{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/magnus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertTokenizerFast, BertModel, BertConfig\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from text_processing import fix_string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Ленивый способ\n",
    "Я возьму предобученый трансформер из **transformers** (буду использовать rubert-tiny) и дообучу его для задачи классификации. Буду использовать основные метрики классификации"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(299, 0, 9.274346984843598, 7.5761680790143)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "new_df = df.copy()\n",
    "new_df[\"text\"] = df[\"text\"].map(fix_string)\n",
    "\n",
    "word_count = new_df[\"text\"].apply(lambda x: len(x.split()))\n",
    "word_count.max(), word_count.min(), word_count.mean(), word_count.std()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text  label\n2241         так? -*- *- *-**    - ** * --    *- *- *-**      1\n4687         надо, мне все надо и чем больше тем лучше:)      1\n6969   - - - - - - - - - - - - - - - - - - - - - - - ...      1\n9176                           не что за - а - за что? )      1\n11389              если было бы так, здесь бы не было их      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2241</th>\n      <td>так? -*- *- *-**    - ** * --    *- *- *-**</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4687</th>\n      <td>надо, мне все надо и чем больше тем лучше:)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6969</th>\n      <td>- - - - - - - - - - - - - - - - - - - - - - - ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9176</th>\n      <td>не что за - а - за что? )</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11389</th>\n      <td>если было бы так, здесь бы не было их</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[word_count == 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Стоит ограничить сообщения 64 словами, в дальнейшем для обработки длинных сообщений можно будет разделить сообщение по 64 слова, и сложить результаты модели на каждой из частей для предсказания класса"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "-33"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df[(word_count > 0) & (word_count <= 64)]\n",
    "len(new_df) - len(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(10576, 1176, 619)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(new_df[\"text\"])\n",
    "Y = np.array(new_df[\"label\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.05, stratify=Y, shuffle=True\n",
    ")\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, stratify=y_train, shuffle=True\n",
    ")\n",
    "\n",
    "len(x_train), len(x_valid), len(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny\", num_labels=2\n",
    ").to(device).train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "weight_path_bert = \"best_bert.pt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magnus/Documents/Testovoe_sber/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from engine import BertDataset, bert_train_epoch, bert_eval_epoch\n",
    "\n",
    "train_data = BertDataset(x_train, y_train, tokenizer)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_data = BertDataset(x_valid, y_valid, tokenizer)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = BertDataset(x_test, y_test, tokenizer)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss 0.5597811366534269 accuracy 0.7113274931907654\n",
      "Val loss 0.4888742936624063 accuracy 0.7627550959587097\n",
      "----------\n",
      "Epoch 2/10\n",
      "Train loss 0.464029548478379 accuracy 0.7853630781173706\n",
      "Val loss 0.4706031502904119 accuracy 0.7644557952880859\n",
      "----------\n",
      "Epoch 3/10\n",
      "Train loss 0.40619960526475024 accuracy 0.8189296126365662\n",
      "Val loss 0.4835069336601206 accuracy 0.7704081535339355\n",
      "----------\n",
      "Epoch 4/10\n",
      "Train loss 0.3595798329285342 accuracy 0.8438917994499207\n",
      "Val loss 0.4942210210940322 accuracy 0.7687074542045593\n",
      "----------\n",
      "Epoch 5/10\n",
      "Train loss 0.3211335728054869 accuracy 0.8653554916381836\n",
      "Val loss 0.5039865545727111 accuracy 0.7823129296302795\n",
      "----------\n",
      "Epoch 6/10\n",
      "Train loss 0.2854191019015215 accuracy 0.8829424977302551\n",
      "Val loss 0.5348457547055708 accuracy 0.7763605117797852\n",
      "----------\n",
      "Epoch 7/10\n",
      "Train loss 0.24640654272057466 accuracy 0.902609646320343\n",
      "Val loss 0.5790631580795791 accuracy 0.7687074542045593\n",
      "----------\n",
      "Epoch 8/10\n",
      "Train loss 0.21839067412881286 accuracy 0.9114977121353149\n",
      "Val loss 0.6131333098117564 accuracy 0.7695578336715698\n",
      "----------\n",
      "Epoch 9/10\n",
      "Train loss 0.1905576176335542 accuracy 0.9242624640464783\n",
      "Val loss 0.6219184589949814 accuracy 0.7678571343421936\n",
      "----------\n",
      "Epoch 10/10\n",
      "Train loss 0.1648862276440444 accuracy 0.936648964881897\n",
      "Val loss 0.6776994199970284 accuracy 0.7661564350128174\n",
      "----------\n",
      "tensor(0.7823, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.best_acc = 0\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model = model.train()\n",
    "    correct_predictions, train_loss = bert_train_epoch(model, optimizer, train_loader, loss_function, device)\n",
    "    print(f'Train loss {train_loss} accuracy {correct_predictions / len(train_data)}')\n",
    "\n",
    "    model = model.eval()\n",
    "    correct_predictions, val_loss = bert_eval_epoch(model, valid_loader, loss_function, device)\n",
    "    current_acc = correct_predictions / len(valid_data)\n",
    "    print(f'Val loss {val_loss} accuracy {current_acc}')\n",
    "\n",
    "    if current_acc > model.best_acc:\n",
    "        model.best_acc = current_acc\n",
    "        torch.save(model, weight_path_bert)\n",
    "\n",
    "    print('-' * 10)\n",
    "\n",
    "print(model.best_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model = torch.load(weight_path_bert).to(device).eval()\n",
    "\n",
    "classes = [\"Чат по Python\", \"Чат по DS\"]\n",
    "\n",
    "def get_bad_prediction(model, data):\n",
    "    predict, target = [], []\n",
    "    bad_prediction = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in data:\n",
    "            input_ids = i[\"input_ids\"].to(device).unsqueeze(0)\n",
    "            attention_mask = i[\"attention_mask\"].to(device).unsqueeze(0)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )[\"logits\"]\n",
    "\n",
    "            predict.append(bool(outputs.argmax()))\n",
    "            target.append(bool(i[\"targets\"]))\n",
    "\n",
    "            if outputs.argmax() != i[\"targets\"]:\n",
    "                bad_prediction.append([i['text'], classes[i[\"targets\"].item()]])\n",
    "    return predict, target, sorted(bad_prediction, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "def print_score(predict, target):\n",
    "    print(\n",
    "        f\"accuracy = {accuracy_score(predict, target)}\\nprecision = {precision_score(predict, target)}\\\n",
    "        \\nrecall = {recall_score(predict, target)}\\nf1 = {f1_score(predict, target)}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Точность модели:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты на train:\n",
      "accuracy = 0.9120650529500757\n",
      "precision = 0.9538409129620167        \n",
      "recall = 0.8947116152740054\n",
      "f1 = 0.9233305853256388\n",
      "худшие примеры:\n",
      "['ясн друг множествен запрос rest api котор ddos ит куч параллельн запрос прокс', 'Чат по DS']\n",
      "['ясл соря', 'Чат по Python']\n",
      "['язык общ пользован имеет вид написа фронтенд бэкенд например', 'Чат по DS']\n",
      "----------\n",
      "Результаты на val:\n",
      "accuracy = 0.782312925170068\n",
      "precision = 0.8453292496171516        \n",
      "recall = 0.7807637906647807\n",
      "f1 = 0.8117647058823528\n",
      "худшие примеры:\n",
      "['ютуб гайд написа бот', 'Чат по Python']\n",
      "['юзер звонк инет привязк дат расшифровк происход то зон юзер', 'Чат по DS']\n",
      "['юза вмест rethinkdb дан хран json', 'Чат по Python']\n",
      "----------\n",
      "Результаты на test:\n",
      "accuracy = 0.7625201938610663\n",
      "precision = 0.8168604651162791        \n",
      "recall = 0.7698630136986301\n",
      "f1 = 0.7926657263751763\n",
      "худшие примеры:\n",
      "['эт фигн хоч сраз валидац ответ', 'Чат по Python']\n",
      "['эт тип лог так принят класт реляцион', 'Чат по Python']\n",
      "['эт стол сложн задач реш прошедш минут', 'Чат по DS']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for name, data in zip([\"train\", \"val\", \"test\"], [train_data, valid_data, test_data]):\n",
    "    predict, target, bad_pred = get_bad_prediction(model, data)\n",
    "\n",
    "    print(f\"Результаты на {name}:\")\n",
    "    print_score(predict, target)\n",
    "    print(\"худшие примеры:\")\n",
    "    for i in bad_pred[:3]:\n",
    "        print(i)\n",
    "\n",
    "    print(\"-\" * 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Выводы\n",
    "Bert оказалась слишком сложной моделью для этой задачи, скорее всего повлияло плохое распределение датасета (неравномерная длина сообщения в трэйне и тесте, более редкие слова встречались чаще в тесте, чем в трэйне) надо было написать или найти сэмплер для текста. Так же можно обучать с маскированием или шумными функциями для текста, так модель быстрее бы запомнила редкие слова и структуру сообщений в чате."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Доработать Берт\n",
    "Возможно модель плохо обучилась, потому что в датасете очень много незнакомых слов, стоит переобучить токенайзер и соответсвенно эмбеддинг"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magnus/Documents/Testovoe_sber/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "(True, False)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 2 ** 14\n",
    "\n",
    "# обучаю новый токенайзер\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "tokenizer = tokenizer.train_new_from_iterator(X, vocab_size)\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# конфиг для модели донора\n",
    "new_config = model.config\n",
    "new_config.vocab_size = vocab_size\n",
    "\n",
    "# изменяю ембединг\n",
    "new_model = BertModel(new_config).to(device)\n",
    "model.bert.embeddings = new_model.embeddings\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "# замораживаю веса\n",
    "model.requires_grad_(False)\n",
    "model.bert.embeddings.requires_grad_(True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "model.bert.embeddings.word_embeddings.weight.requires_grad, model.bert.encoder.layer[0].attention.self.query.weight.requires_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_data = BertDataset(x_train, y_train, model.tokenizer)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_data = BertDataset(x_valid, y_valid, model.tokenizer)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = BertDataset(x_test, y_test, model.tokenizer)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 эпохи для обучения нового эмбединга"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train loss 0.6174467542348219 accuracy 0.6730332374572754\n",
      "----------\n",
      "Epoch 2/2\n",
      "Train loss 0.42323020574004133 accuracy 0.8142964839935303\n",
      "----------\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "model.best_acc = 0\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    # model = model.train()\n",
    "    correct_predictions, train_loss = bert_train_epoch(model, optimizer, train_loader, loss_function, device)\n",
    "    print(f'Train loss {train_loss} accuracy {correct_predictions / len(train_data)}')\n",
    "\n",
    "    print('-' * 10)\n",
    "\n",
    "print(model.best_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь полноценное обучение"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_path_new_bert = \"best_bert_new_tokenizer.pt\"\n",
    "\n",
    "model.requires_grad_(True)\n",
    "model.bert.encoder.layer[0].attention.self.query.weight.requires_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss 0.34526755714312807 accuracy 0.8519288897514343\n",
      "Val loss 0.47905171753184217 accuracy 0.7967686653137207\n",
      "----------\n",
      "Epoch 2/10\n",
      "Train loss 0.2524181635354317 accuracy 0.9000567197799683\n",
      "Val loss 0.5036064687411528 accuracy 0.7874149680137634\n",
      "----------\n",
      "Epoch 3/10\n",
      "Train loss 0.20271579368651454 accuracy 0.9200075268745422\n",
      "Val loss 0.5400593685539993 accuracy 0.7908163070678711\n",
      "----------\n",
      "Epoch 4/10\n",
      "Train loss 0.162114782336361 accuracy 0.9397692680358887\n",
      "Val loss 0.5807000078462266 accuracy 0.7908163070678711\n",
      "----------\n",
      "Epoch 5/10\n",
      "Train loss 0.12956458012433933 accuracy 0.9506429433822632\n",
      "Val loss 0.637724176149916 accuracy 0.7942176461219788\n",
      "----------\n",
      "Epoch 6/10\n",
      "Train loss 0.10219696921376728 accuracy 0.9617056846618652\n",
      "Val loss 0.7278462199594926 accuracy 0.7908163070678711\n",
      "----------\n",
      "Epoch 7/10\n",
      "Train loss 0.0849674245245394 accuracy 0.9697427749633789\n",
      "Val loss 0.772433947469737 accuracy 0.784013569355011\n",
      "----------\n",
      "Epoch 8/10\n",
      "Train loss 0.0696925594754596 accuracy 0.9763615131378174\n",
      "Val loss 0.799407901204619 accuracy 0.7916666269302368\n",
      "----------\n",
      "Epoch 9/10\n",
      "Train loss 0.06470978763954835 accuracy 0.9765506386756897\n",
      "Val loss 0.9462341972425379 accuracy 0.7721088528633118\n",
      "----------\n",
      "Epoch 10/10\n",
      "Train loss 0.05068440727714122 accuracy 0.9825075268745422\n",
      "Val loss 0.8468653095030302 accuracy 0.7916666269302368\n",
      "----------\n",
      "tensor(0.7968, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.best_acc = 0\n",
    "model.tokenizer = tokenizer\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model = model.train()\n",
    "    correct_predictions, train_loss = bert_train_epoch(model, optimizer, train_loader, loss_function, device)\n",
    "    print(f'Train loss {train_loss} accuracy {correct_predictions / len(train_data)}')\n",
    "\n",
    "    model = model.eval()\n",
    "    correct_predictions, val_loss = bert_eval_epoch(model, valid_loader, loss_function, device)\n",
    "    current_acc = correct_predictions / len(valid_data)\n",
    "    print(f'Val loss {val_loss} accuracy {current_acc}')\n",
    "\n",
    "    if current_acc > model.best_acc:\n",
    "        model.best_acc = current_acc\n",
    "        torch.save(model, weight_path_new_bert)\n",
    "\n",
    "    print('-' * 10)\n",
    "\n",
    "print(model.best_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты на train:\n",
      "accuracy = 0.9244515885022693\n",
      "precision = 0.9460057911769716        \n",
      "recall = 0.9201457919151756\n",
      "f1 = 0.932896615436298\n",
      "худшие примеры:\n",
      "['ясн друг множествен запрос rest api котор ddos ит куч параллельн запрос прокс', 'Чат по DS']\n",
      "['язык общ пользован имеет вид написа фронтенд бэкенд например', 'Чат по DS']\n",
      "['эт эт одн', 'Чат по Python']\n",
      "----------\n",
      "Результаты на val:\n",
      "accuracy = 0.7967687074829932\n",
      "precision = 0.8529862174578867        \n",
      "recall = 0.7957142857142857\n",
      "f1 = 0.8233555062823356\n",
      "худшие примеры:\n",
      "['ээээ маленьк так слов слыша чита пойд гугл всем спасиб', 'Чат по DS']\n",
      "['эффект начина работа ощутим значен крошечн', 'Чат по DS']\n",
      "['эт чита смысл', 'Чат по Python']\n",
      "----------\n",
      "Результаты на test:\n",
      "accuracy = 0.7964458804523424\n",
      "precision = 0.8313953488372093        \n",
      "recall = 0.807909604519774\n",
      "f1 = 0.8194842406876792\n",
      "худшие примеры:\n",
      "['юзер аутист умеет оптимизирова', 'Чат по Python']\n",
      "['ээээ регион чувак писа сюд стнегион проблем реша саппорт', 'Чат по DS']\n",
      "['эт так прост модел разделен класс тип svm послаб', 'Чат по DS']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(weight_path_new_bert).to(device)\n",
    "\n",
    "for name, data in zip([\"train\", \"val\", \"test\"], [train_data, valid_data, test_data]):\n",
    "    predict, target, bad_pred = get_bad_prediction(model, data)\n",
    "\n",
    "    print(f\"Результаты на {name}:\")\n",
    "    print_score(predict, target)\n",
    "    print(\"худшие примеры:\")\n",
    "    for i in bad_pred[:3]:\n",
    "        print(i)\n",
    "\n",
    "    print(\"-\" * 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Выводы\n",
    "Результат оказался лучше, но не таким как я ожидал, думаю с сэмплером и некоторыми селф-супервайз подходами результат мог быть лучше."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Лёгкая модель\n",
    "Стоит попробовать сделать лёгкую модель, потому что предыдущие решения могут быть излишни"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нужно построить токенайзер"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from engine import SimpleDataset, SimpleClassificator, simple_train_epoch, simple_eval_epoch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def yield_tokens():\n",
    "    for i in new_df[\"text\"]:\n",
    "        yield i.split()\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(), specials=[\"_\"])  # specials for padding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_tokens = 64"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_data = SimpleDataset(x_train, y_train, vocab, max_tokens)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_data = SimpleDataset(x_valid, y_valid, vocab, max_tokens)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = SimpleDataset(x_test, y_test, vocab, max_tokens)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "model = SimpleClassificator(len(vocab), max_tokens).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "Train loss 0.6903196163924343 accuracy 0.5387669801712036\n",
      "Val loss 0.6864371836185456 accuracy 0.555272102355957\n",
      "----------\n",
      "Epoch 2/500\n",
      "Train loss 0.6862077217504202 accuracy 0.5566376447677612\n",
      "Val loss 0.6859510004520416 accuracy 0.5612244606018066\n",
      "----------\n",
      "Epoch 3/500\n",
      "Train loss 0.6826295342790075 accuracy 0.5678895115852356\n",
      "Val loss 0.6878229081630707 accuracy 0.5612244606018066\n",
      "----------\n",
      "Epoch 4/500\n",
      "Train loss 0.6754190239561609 accuracy 0.5816943645477295\n",
      "Val loss 0.6839608490467072 accuracy 0.5705782175064087\n",
      "----------\n",
      "Epoch 5/500\n",
      "Train loss 0.6660277225885046 accuracy 0.5941754579544067\n",
      "Val loss 0.6746254742145539 accuracy 0.5807822942733765\n",
      "----------\n",
      "Epoch 6/500\n",
      "Train loss 0.6514588163559696 accuracy 0.6078857779502869\n",
      "Val loss 0.6674558401107789 accuracy 0.6113945245742798\n",
      "----------\n",
      "Epoch 7/500\n",
      "Train loss 0.6367719115981136 accuracy 0.6193267703056335\n",
      "Val loss 0.6618392765522003 accuracy 0.6139455437660217\n",
      "----------\n",
      "Epoch 8/500\n",
      "Train loss 0.6188933325101094 accuracy 0.6349281072616577\n",
      "Val loss 0.6584762930870056 accuracy 0.6122449040412903\n",
      "----------\n",
      "Epoch 9/500\n",
      "Train loss 0.5964481995766422 accuracy 0.6423032879829407\n",
      "Val loss 0.6568465113639832 accuracy 0.6284013390541077\n",
      "----------\n",
      "Epoch 10/500\n",
      "Train loss 0.5859657906624208 accuracy 0.6535552144050598\n",
      "Val loss 0.6559316575527191 accuracy 0.6488094925880432\n",
      "----------\n",
      "Epoch 11/500\n",
      "Train loss 0.5651938599276255 accuracy 0.66329425573349\n",
      "Val loss 0.6453525722026825 accuracy 0.6692176461219788\n",
      "----------\n",
      "Epoch 12/500\n",
      "Train loss 0.5569075834320252 accuracy 0.6717094779014587\n",
      "Val loss 0.6489360809326172 accuracy 0.6777210831642151\n",
      "----------\n",
      "Epoch 13/500\n",
      "Train loss 0.5334590727306274 accuracy 0.6814485192298889\n",
      "Val loss 0.6636556088924408 accuracy 0.656462550163269\n",
      "----------\n",
      "Epoch 14/500\n",
      "Train loss 0.5238332802272705 accuracy 0.6875945329666138\n",
      "Val loss 0.6675381511449814 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 15/500\n",
      "Train loss 0.5084082268806825 accuracy 0.6917548775672913\n",
      "Val loss 0.7407089591026306 accuracy 0.6488094925880432\n",
      "----------\n",
      "Epoch 16/500\n",
      "Train loss 0.4951754729431796 accuracy 0.6974281072616577\n",
      "Val loss 0.6663066416978836 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 17/500\n",
      "Train loss 0.49332789125212706 accuracy 0.6997919678688049\n",
      "Val loss 0.7608777046203613 accuracy 0.6437074542045593\n",
      "----------\n",
      "Epoch 18/500\n",
      "Train loss 0.48059490789850073 accuracy 0.7048032879829407\n",
      "Val loss 0.6970680385828019 accuracy 0.6726190447807312\n",
      "----------\n",
      "Epoch 19/500\n",
      "Train loss 0.4723748614989131 accuracy 0.7132185697555542\n",
      "Val loss 0.8117398023605347 accuracy 0.6598639488220215\n",
      "----------\n",
      "Epoch 20/500\n",
      "Train loss 0.46672885101961803 accuracy 0.7116111516952515\n",
      "Val loss 0.7527793705463409 accuracy 0.6539115309715271\n",
      "----------\n",
      "Epoch 21/500\n",
      "Train loss 0.45789552417146157 accuracy 0.7167170643806458\n",
      "Val loss 0.9275980293750763 accuracy 0.656462550163269\n",
      "----------\n",
      "Epoch 22/500\n",
      "Train loss 0.4507496550858739 accuracy 0.7180408239364624\n",
      "Val loss 0.8590773522853852 accuracy 0.6768707633018494\n",
      "----------\n",
      "Epoch 23/500\n",
      "Train loss 0.44833844756505575 accuracy 0.722484827041626\n",
      "Val loss 0.8665696501731872 accuracy 0.6794217824935913\n",
      "----------\n",
      "Epoch 24/500\n",
      "Train loss 0.4425645704010883 accuracy 0.7260779142379761\n",
      "Val loss 0.963302481174469 accuracy 0.668367326259613\n",
      "----------\n",
      "Epoch 25/500\n",
      "Train loss 0.42760667779359474 accuracy 0.7289144992828369\n",
      "Val loss 1.2735958099365234 accuracy 0.6649659872055054\n",
      "----------\n",
      "Epoch 26/500\n",
      "Train loss 0.42725952454360133 accuracy 0.7325075268745422\n",
      "Val loss 1.1697999656200408 accuracy 0.6615645885467529\n",
      "----------\n",
      "Epoch 27/500\n",
      "Train loss 0.42211396220218705 accuracy 0.7399773001670837\n",
      "Val loss 1.1174663603305817 accuracy 0.6581632494926453\n",
      "----------\n",
      "Epoch 28/500\n",
      "Train loss 0.43383180161556567 accuracy 0.7273070812225342\n",
      "Val loss 0.9322867870330811 accuracy 0.6556122303009033\n",
      "----------\n",
      "Epoch 29/500\n",
      "Train loss 0.4237361263079816 accuracy 0.730711042881012\n",
      "Val loss 1.062278813123703 accuracy 0.6675170063972473\n",
      "----------\n",
      "Epoch 30/500\n",
      "Train loss 0.4302474942552038 accuracy 0.7270234227180481\n",
      "Val loss 0.9104804813861846 accuracy 0.6615645885467529\n",
      "----------\n",
      "Epoch 31/500\n",
      "Train loss 0.43205355484801605 accuracy 0.7295763492584229\n",
      "Val loss 0.9235248386859893 accuracy 0.6462584733963013\n",
      "----------\n",
      "Epoch 32/500\n",
      "Train loss 0.43201636621750983 accuracy 0.7285363078117371\n",
      "Val loss 1.1056835114955903 accuracy 0.6666666269302368\n",
      "----------\n",
      "Epoch 33/500\n",
      "Train loss 0.4249428995402463 accuracy 0.7277798652648926\n",
      "Val loss 1.376372742652893 accuracy 0.6513605117797852\n",
      "----------\n",
      "Epoch 34/500\n",
      "Train loss 0.4164335189095463 accuracy 0.736856997013092\n",
      "Val loss 1.4427683472633361 accuracy 0.6666666269302368\n",
      "----------\n",
      "Epoch 35/500\n",
      "Train loss 0.41897289042013236 accuracy 0.7322238683700562\n",
      "Val loss 1.6191968083381654 accuracy 0.6675170063972473\n",
      "----------\n",
      "Epoch 36/500\n",
      "Train loss 0.4191276273095464 accuracy 0.733453094959259\n",
      "Val loss 1.302916795015335 accuracy 0.6666666269302368\n",
      "----------\n",
      "Epoch 37/500\n",
      "Train loss 0.40989778558891937 accuracy 0.7374243140220642\n",
      "Val loss 1.7917186737060546 accuracy 0.6530612111091614\n",
      "----------\n",
      "Epoch 38/500\n",
      "Train loss 0.41655682942953454 accuracy 0.7351550459861755\n",
      "Val loss 1.7171986103057861 accuracy 0.6658163070678711\n",
      "----------\n",
      "Epoch 39/500\n",
      "Train loss 0.42020678053419275 accuracy 0.7314674258232117\n",
      "Val loss 1.6283934473991395 accuracy 0.6641156077384949\n",
      "----------\n",
      "Epoch 40/500\n",
      "Train loss 0.41695108147988835 accuracy 0.7336421608924866\n",
      "Val loss 1.532514178752899 accuracy 0.6632652878761292\n",
      "----------\n",
      "Epoch 41/500\n",
      "Train loss 0.41808920093329555 accuracy 0.7285363078117371\n",
      "Val loss 1.5238475501537323 accuracy 0.659013569355011\n",
      "----------\n",
      "Epoch 42/500\n",
      "Train loss 0.4116745889186859 accuracy 0.7347768545150757\n",
      "Val loss 1.3939221262931825 accuracy 0.6785714030265808\n",
      "----------\n",
      "Epoch 43/500\n",
      "Train loss 0.41253531445939856 accuracy 0.734398603439331\n",
      "Val loss 1.6806578874588012 accuracy 0.6666666269302368\n",
      "----------\n",
      "Epoch 44/500\n",
      "Train loss 0.4023495413452746 accuracy 0.7402609586715698\n",
      "Val loss 1.9172255039215087 accuracy 0.6488094925880432\n",
      "----------\n",
      "Epoch 45/500\n",
      "Train loss 0.41040961677769583 accuracy 0.7373297810554504\n",
      "Val loss 1.6191356539726258 accuracy 0.6556122303009033\n",
      "----------\n",
      "Epoch 46/500\n",
      "Train loss 0.41297506316598637 accuracy 0.7321293354034424\n",
      "Val loss 1.9409237861633302 accuracy 0.6641156077384949\n",
      "----------\n",
      "Epoch 47/500\n",
      "Train loss 0.4172516522637333 accuracy 0.7295763492584229\n",
      "Val loss 1.5875962734222413 accuracy 0.6598639488220215\n",
      "----------\n",
      "Epoch 48/500\n",
      "Train loss 0.4151930496635207 accuracy 0.7354387044906616\n",
      "Val loss 1.7872926950454713 accuracy 0.6607142686843872\n",
      "----------\n",
      "Epoch 49/500\n",
      "Train loss 0.410335075065314 accuracy 0.7372352480888367\n",
      "Val loss 1.5254339337348939 accuracy 0.6496598720550537\n",
      "----------\n",
      "Epoch 50/500\n",
      "Train loss 0.4106866290052253 accuracy 0.7369515895843506\n",
      "Val loss 1.8123005270957946 accuracy 0.656462550163269\n",
      "----------\n",
      "Epoch 51/500\n",
      "Train loss 0.4115914644965206 accuracy 0.7344931364059448\n",
      "Val loss 1.6817197799682617 accuracy 0.6641156077384949\n",
      "----------\n",
      "Epoch 52/500\n",
      "Train loss 0.4123802554894643 accuracy 0.7339258193969727\n",
      "Val loss 1.9059255361557006 accuracy 0.6641156077384949\n",
      "----------\n",
      "Epoch 53/500\n",
      "Train loss 0.4033040655664651 accuracy 0.7410173416137695\n",
      "Val loss 1.587196671962738 accuracy 0.6649659872055054\n",
      "----------\n",
      "Epoch 54/500\n",
      "Train loss 0.4113502039248685 accuracy 0.7370461225509644\n",
      "Val loss 1.732734316587448 accuracy 0.6743196845054626\n",
      "----------\n",
      "Epoch 55/500\n",
      "Train loss 0.40978445812880276 accuracy 0.7376134395599365\n",
      "Val loss 1.3922264337539674 accuracy 0.682823121547699\n",
      "----------\n",
      "Epoch 56/500\n",
      "Train loss 0.41428565440407716 accuracy 0.7318456768989563\n",
      "Val loss 2.1866940021514893 accuracy 0.6862244606018066\n",
      "----------\n",
      "Epoch 57/500\n",
      "Train loss 0.4208227988467159 accuracy 0.7303327918052673\n",
      "Val loss 1.5279763698577882 accuracy 0.670918345451355\n",
      "----------\n",
      "Epoch 58/500\n",
      "Train loss 0.40906499846872074 accuracy 0.7358168959617615\n",
      "Val loss 1.6896270871162415 accuracy 0.6700680255889893\n",
      "----------\n",
      "Epoch 59/500\n",
      "Train loss 0.41341710198356446 accuracy 0.7363842129707336\n",
      "Val loss 1.4769895792007446 accuracy 0.668367326259613\n",
      "----------\n",
      "Epoch 60/500\n",
      "Train loss 0.42175291137522963 accuracy 0.7302382588386536\n",
      "Val loss 1.2946734726428986 accuracy 0.6777210831642151\n",
      "----------\n",
      "Epoch 61/500\n",
      "Train loss 0.4101397014525999 accuracy 0.7386535406112671\n",
      "Val loss 1.2607218146324157 accuracy 0.6760203838348389\n",
      "----------\n",
      "Epoch 62/500\n",
      "Train loss 0.4092206387634737 accuracy 0.7369515895843506\n",
      "Val loss 1.5325370609760285 accuracy 0.6717686653137207\n",
      "----------\n",
      "Epoch 63/500\n",
      "Train loss 0.41181257690291806 accuracy 0.7346822619438171\n",
      "Val loss 1.4614649355411529 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 64/500\n",
      "Train loss 0.41212133255349587 accuracy 0.7313728928565979\n",
      "Val loss 2.0273393392562866 accuracy 0.670918345451355\n",
      "----------\n",
      "Epoch 65/500\n",
      "Train loss 0.4088036435914327 accuracy 0.7374243140220642\n",
      "Val loss 1.5080483675003051 accuracy 0.6692176461219788\n",
      "----------\n",
      "Epoch 66/500\n",
      "Train loss 0.4082097931798682 accuracy 0.7340204119682312\n",
      "Val loss 1.8259909391403197 accuracy 0.6692176461219788\n",
      "----------\n",
      "Epoch 67/500\n",
      "Train loss 0.40578558753771954 accuracy 0.7370461225509644\n",
      "Val loss 1.4225159645080567 accuracy 0.6598639488220215\n",
      "----------\n",
      "Epoch 68/500\n",
      "Train loss 0.4109619040805173 accuracy 0.7330748438835144\n",
      "Val loss 1.8083541750907899 accuracy 0.6556122303009033\n",
      "----------\n",
      "Epoch 69/500\n",
      "Train loss 0.4158408099628357 accuracy 0.7296709418296814\n",
      "Val loss 1.337436020374298 accuracy 0.6743196845054626\n",
      "----------\n",
      "Epoch 70/500\n",
      "Train loss 0.4110201169927436 accuracy 0.7352495789527893\n",
      "Val loss 1.5869869351387025 accuracy 0.6777210831642151\n",
      "----------\n",
      "Epoch 71/500\n",
      "Train loss 0.412050706794463 accuracy 0.736856997013092\n",
      "Val loss 1.5507874250411988 accuracy 0.6768707633018494\n",
      "----------\n",
      "Epoch 72/500\n",
      "Train loss 0.40996800488736257 accuracy 0.7364788055419922\n",
      "Val loss 1.657724666595459 accuracy 0.6658163070678711\n",
      "----------\n",
      "Epoch 73/500\n",
      "Train loss 0.41040163442312955 accuracy 0.7382752895355225\n",
      "Val loss 1.8100752353668212 accuracy 0.6785714030265808\n",
      "----------\n",
      "Epoch 74/500\n",
      "Train loss 0.4162113472639796 accuracy 0.7288199663162231\n",
      "Val loss 1.3385213017463684 accuracy 0.6811224222183228\n",
      "----------\n",
      "Epoch 75/500\n",
      "Train loss 0.4114815237292324 accuracy 0.7367624640464783\n",
      "Val loss 1.9754077076911927 accuracy 0.6904761791229248\n",
      "----------\n",
      "Epoch 76/500\n",
      "Train loss 0.41222755916147347 accuracy 0.733453094959259\n",
      "Val loss 2.1048134088516237 accuracy 0.6743196845054626\n",
      "----------\n",
      "Epoch 77/500\n",
      "Train loss 0.4103136741253267 accuracy 0.7369515895843506\n",
      "Val loss 1.8346281886100768 accuracy 0.6887754797935486\n",
      "----------\n",
      "Epoch 78/500\n",
      "Train loss 0.41818024774631823 accuracy 0.7312783598899841\n",
      "Val loss 1.6136985063552856 accuracy 0.6836734414100647\n",
      "----------\n",
      "Epoch 79/500\n",
      "Train loss 0.4157252067542938 accuracy 0.7311837673187256\n",
      "Val loss 2.247391963005066 accuracy 0.6938775181770325\n",
      "----------\n",
      "Epoch 80/500\n",
      "Train loss 0.41084454792091646 accuracy 0.7373297810554504\n",
      "Val loss 1.44782897233963 accuracy 0.670918345451355\n",
      "----------\n",
      "Epoch 81/500\n",
      "Train loss 0.4102358771375863 accuracy 0.7371406555175781\n",
      "Val loss 1.4609907865524292 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 82/500\n",
      "Train loss 0.4158681749579418 accuracy 0.7320347428321838\n",
      "Val loss 1.6805078327655791 accuracy 0.680272102355957\n",
      "----------\n",
      "Epoch 83/500\n",
      "Train loss 0.40897308379770764 accuracy 0.7363842129707336\n",
      "Val loss 2.024364060163498 accuracy 0.659013569355011\n",
      "----------\n",
      "Epoch 84/500\n",
      "Train loss 0.404174865010273 accuracy 0.7413955926895142\n",
      "Val loss 1.9981656074523926 accuracy 0.6794217824935913\n",
      "----------\n",
      "Epoch 85/500\n",
      "Train loss 0.40660192916192206 accuracy 0.7381807565689087\n",
      "Val loss 2.0631974577903747 accuracy 0.6785714030265808\n",
      "----------\n",
      "Epoch 86/500\n",
      "Train loss 0.4123456043651305 accuracy 0.7339258193969727\n",
      "Val loss 1.8323915243148803 accuracy 0.680272102355957\n",
      "----------\n",
      "Epoch 87/500\n",
      "Train loss 0.40354547766317805 accuracy 0.7415847182273865\n",
      "Val loss 2.5787131667137144 accuracy 0.6794217824935913\n",
      "----------\n",
      "Epoch 88/500\n",
      "Train loss 0.40843067004019956 accuracy 0.7347768545150757\n",
      "Val loss 1.5522334158420563 accuracy 0.6760203838348389\n",
      "----------\n",
      "Epoch 89/500\n",
      "Train loss 0.40116316295531856 accuracy 0.7390317320823669\n",
      "Val loss 1.896331000328064 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 90/500\n",
      "Train loss 0.4049980008458517 accuracy 0.739315390586853\n",
      "Val loss 1.42812979221344 accuracy 0.6819728016853333\n",
      "----------\n",
      "Epoch 91/500\n",
      "Train loss 0.40530455507427815 accuracy 0.7401663661003113\n",
      "Val loss 2.2894548416137694 accuracy 0.6717686653137207\n",
      "----------\n",
      "Epoch 92/500\n",
      "Train loss 0.4202680049172367 accuracy 0.7279689311981201\n",
      "Val loss 1.2365454196929933 accuracy 0.6760203838348389\n",
      "----------\n",
      "Epoch 93/500\n",
      "Train loss 0.4105899998222489 accuracy 0.7357223629951477\n",
      "Val loss 1.6042634189128875 accuracy 0.6811224222183228\n",
      "----------\n",
      "Epoch 94/500\n",
      "Train loss 0.4088066776832902 accuracy 0.7364788055419922\n",
      "Val loss 2.150301933288574 accuracy 0.6896258592605591\n",
      "----------\n",
      "Epoch 95/500\n",
      "Train loss 0.40317073715738505 accuracy 0.7357223629951477\n",
      "Val loss 1.647653204202652 accuracy 0.6887754797935486\n",
      "----------\n",
      "Epoch 96/500\n",
      "Train loss 0.4122956064810236 accuracy 0.7338312864303589\n",
      "Val loss 2.0380996465682983 accuracy 0.6768707633018494\n",
      "----------\n",
      "Epoch 97/500\n",
      "Train loss 0.4066394563898983 accuracy 0.7399773001670837\n",
      "Val loss 1.8100827574729919 accuracy 0.6904761791229248\n",
      "----------\n",
      "Epoch 98/500\n",
      "Train loss 0.40219382205641413 accuracy 0.7413010597229004\n",
      "Val loss 1.7489930391311646 accuracy 0.680272102355957\n",
      "----------\n",
      "Epoch 99/500\n",
      "Train loss 0.4143610808504633 accuracy 0.7390317320823669\n",
      "Val loss 0.9730427682399749 accuracy 0.6726190447807312\n",
      "----------\n",
      "Epoch 100/500\n",
      "Train loss 0.41419828476676024 accuracy 0.736856997013092\n",
      "Val loss 1.3220876216888429 accuracy 0.6734693646430969\n",
      "----------\n",
      "Epoch 101/500\n",
      "Train loss 0.40741726073874046 accuracy 0.7412064671516418\n",
      "Val loss 1.1953153729438781 accuracy 0.6726190447807312\n",
      "----------\n",
      "Epoch 102/500\n",
      "Train loss 0.4029945785022644 accuracy 0.7437594532966614\n",
      "Val loss 1.2382439434528352 accuracy 0.6904761791229248\n",
      "----------\n",
      "Epoch 103/500\n",
      "Train loss 0.41121534985232067 accuracy 0.7325075268745422\n",
      "Val loss 3.186093640327454 accuracy 0.6785714030265808\n",
      "----------\n",
      "Epoch 104/500\n",
      "Train loss 0.4133233485451664 accuracy 0.733453094959259\n",
      "Val loss 1.646273934841156 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 105/500\n",
      "Train loss 0.40960444312497796 accuracy 0.7345877289772034\n",
      "Val loss 1.5953294098377229 accuracy 0.7040815949440002\n",
      "----------\n",
      "Epoch 106/500\n",
      "Train loss 0.40904020329555835 accuracy 0.7362896800041199\n",
      "Val loss 1.9791158199310304 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 107/500\n",
      "Train loss 0.40296465014836874 accuracy 0.7375189065933228\n",
      "Val loss 1.720937144756317 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 108/500\n",
      "Train loss 0.41250907441219653 accuracy 0.731656551361084\n",
      "Val loss 1.85941801071167 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 109/500\n",
      "Train loss 0.40457542366292104 accuracy 0.7387480735778809\n",
      "Val loss 1.4741173028945922 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 110/500\n",
      "Train loss 0.40100782068378954 accuracy 0.7424356937408447\n",
      "Val loss 1.9470503985881806 accuracy 0.6862244606018066\n",
      "----------\n",
      "Epoch 111/500\n",
      "Train loss 0.41487091995147335 accuracy 0.7311837673187256\n",
      "Val loss 1.8121016025543213 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 112/500\n",
      "Train loss 0.40697761292917184 accuracy 0.7355332374572754\n",
      "Val loss 2.238367486000061 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 113/500\n",
      "Train loss 0.40662971892988825 accuracy 0.738369882106781\n",
      "Val loss 1.943683397769928 accuracy 0.6887754797935486\n",
      "----------\n",
      "Epoch 114/500\n",
      "Train loss 0.4050412267805582 accuracy 0.7375189065933228\n",
      "Val loss 2.1793475568294527 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 115/500\n",
      "Train loss 0.40605236500142566 accuracy 0.7357223629951477\n",
      "Val loss 1.8319554805755616 accuracy 0.6862244606018066\n",
      "----------\n",
      "Epoch 116/500\n",
      "Train loss 0.40405639395656356 accuracy 0.7379916310310364\n",
      "Val loss 1.6888168692588805 accuracy 0.6836734414100647\n",
      "----------\n",
      "Epoch 117/500\n",
      "Train loss 0.41469557709004506 accuracy 0.7349659204483032\n",
      "Val loss 1.6419583201408385 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 118/500\n",
      "Train loss 0.4078628550092858 accuracy 0.736856997013092\n",
      "Val loss 2.257546079158783 accuracy 0.668367326259613\n",
      "----------\n",
      "Epoch 119/500\n",
      "Train loss 0.4093585646296122 accuracy 0.7369515895843506\n",
      "Val loss 1.6886568665504456 accuracy 0.6819728016853333\n",
      "----------\n",
      "Epoch 120/500\n",
      "Train loss 0.4003587857068303 accuracy 0.7454614043235779\n",
      "Val loss 2.078974151611328 accuracy 0.6896258592605591\n",
      "----------\n",
      "Epoch 121/500\n",
      "Train loss 0.40537061246044664 accuracy 0.7395045161247253\n",
      "Val loss 2.157196283340454 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 122/500\n",
      "Train loss 0.41140899600752867 accuracy 0.7356278300285339\n",
      "Val loss 1.6641331493854523 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 123/500\n",
      "Train loss 0.4047103685786925 accuracy 0.7409228086471558\n",
      "Val loss 1.519485878944397 accuracy 0.6794217824935913\n",
      "----------\n",
      "Epoch 124/500\n",
      "Train loss 0.4118608746183924 accuracy 0.7375189065933228\n",
      "Val loss 1.7143361687660217 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 125/500\n",
      "Train loss 0.4046447632542576 accuracy 0.7369515895843506\n",
      "Val loss 1.6099146723747253 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 126/500\n",
      "Train loss 0.40832035692341356 accuracy 0.7361005544662476\n",
      "Val loss 1.5485386967658996 accuracy 0.6904761791229248\n",
      "----------\n",
      "Epoch 127/500\n",
      "Train loss 0.4073744635265994 accuracy 0.7374243140220642\n",
      "Val loss 1.916531538963318 accuracy 0.6785714030265808\n",
      "----------\n",
      "Epoch 128/500\n",
      "Train loss 0.40387272906590654 accuracy 0.7395045161247253\n",
      "Val loss 1.6577926635742188 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 129/500\n",
      "Train loss 0.41114574598978804 accuracy 0.7346822619438171\n",
      "Val loss 1.6871274948120116 accuracy 0.668367326259613\n",
      "----------\n",
      "Epoch 130/500\n",
      "Train loss 0.40196966945406903 accuracy 0.7387480735778809\n",
      "Val loss 2.32349054813385 accuracy 0.6760203838348389\n",
      "----------\n",
      "Epoch 131/500\n",
      "Train loss 0.40481919768344926 accuracy 0.7399773001670837\n",
      "Val loss 3.569078242778778 accuracy 0.6649659872055054\n",
      "----------\n",
      "Epoch 132/500\n",
      "Train loss 0.4167985101062131 accuracy 0.7377079725265503\n",
      "Val loss 1.4732666015625 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 133/500\n",
      "Train loss 0.41397195982645796 accuracy 0.7312783598899841\n",
      "Val loss 2.728387546539307 accuracy 0.680272102355957\n",
      "----------\n",
      "Epoch 134/500\n",
      "Train loss 0.4130415262946163 accuracy 0.7333585023880005\n",
      "Val loss 1.7733866930007935 accuracy 0.6785714030265808\n",
      "----------\n",
      "Epoch 135/500\n",
      "Train loss 0.40921065857611505 accuracy 0.738369882106781\n",
      "Val loss 1.919775915145874 accuracy 0.6887754797935486\n",
      "----------\n",
      "Epoch 136/500\n",
      "Train loss 0.41253621642848093 accuracy 0.7369515895843506\n",
      "Val loss 1.5397324800491332 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 137/500\n",
      "Train loss 0.41301845211580573 accuracy 0.7347768545150757\n",
      "Val loss 1.9252537369728089 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 138/500\n",
      "Train loss 0.4076219093368714 accuracy 0.7372352480888367\n",
      "Val loss 2.2701878905296327 accuracy 0.6887754797935486\n",
      "----------\n",
      "Epoch 139/500\n",
      "Train loss 0.41105241911957063 accuracy 0.7346822619438171\n",
      "Val loss 2.3129186749458315 accuracy 0.6811224222183228\n",
      "----------\n",
      "Epoch 140/500\n",
      "Train loss 0.41104983385786953 accuracy 0.7347768545150757\n",
      "Val loss 2.1107606887817383 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 141/500\n",
      "Train loss 0.40933371200618973 accuracy 0.7337367534637451\n",
      "Val loss 2.2934789061546326 accuracy 0.6938775181770325\n",
      "----------\n",
      "Epoch 142/500\n",
      "Train loss 0.40649600560406607 accuracy 0.7370461225509644\n",
      "Val loss 1.9125003457069396 accuracy 0.6819728016853333\n",
      "----------\n",
      "Epoch 143/500\n",
      "Train loss 0.41138607802161253 accuracy 0.7377079725265503\n",
      "Val loss 1.407827579975128 accuracy 0.682823121547699\n",
      "----------\n",
      "Epoch 144/500\n",
      "Train loss 0.4077060046684311 accuracy 0.7392208576202393\n",
      "Val loss 1.769714593887329 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 145/500\n",
      "Train loss 0.4043308934533452 accuracy 0.7405446171760559\n",
      "Val loss 2.457536447048187 accuracy 0.682823121547699\n",
      "----------\n",
      "Epoch 146/500\n",
      "Train loss 0.4079954893474119 accuracy 0.7336421608924866\n",
      "Val loss 2.507754755020142 accuracy 0.6819728016853333\n",
      "----------\n",
      "Epoch 147/500\n",
      "Train loss 0.4086087195270033 accuracy 0.7356278300285339\n",
      "Val loss 2.7222400665283204 accuracy 0.6819728016853333\n",
      "----------\n",
      "Epoch 148/500\n",
      "Train loss 0.4108006297105766 accuracy 0.734114944934845\n",
      "Val loss 1.9067196011543275 accuracy 0.680272102355957\n",
      "----------\n",
      "Epoch 149/500\n",
      "Train loss 0.40690460980656634 accuracy 0.7361951470375061\n",
      "Val loss 2.3206706285476684 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 150/500\n",
      "Train loss 0.4086237215852163 accuracy 0.7348713874816895\n",
      "Val loss 2.45614892244339 accuracy 0.6751700639724731\n",
      "----------\n",
      "Epoch 151/500\n",
      "Train loss 0.412043069500521 accuracy 0.7387480735778809\n",
      "Val loss 1.7269302010536194 accuracy 0.6717686653137207\n",
      "----------\n",
      "Epoch 152/500\n",
      "Train loss 0.4063845151160137 accuracy 0.7407336831092834\n",
      "Val loss 2.0821284294128417 accuracy 0.6734693646430969\n",
      "----------\n",
      "Epoch 153/500\n",
      "Train loss 0.40853493615805386 accuracy 0.7325075268745422\n",
      "Val loss 1.721388590335846 accuracy 0.6819728016853333\n",
      "----------\n",
      "Epoch 154/500\n",
      "Train loss 0.4085227746561349 accuracy 0.7414901256561279\n",
      "Val loss 2.355084276199341 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 155/500\n",
      "Train loss 0.4037460268261921 accuracy 0.7403554916381836\n",
      "Val loss 1.9534943103790283 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 156/500\n",
      "Train loss 0.4101439746747534 accuracy 0.7348713874816895\n",
      "Val loss 1.7088740587234497 accuracy 0.6870748400688171\n",
      "----------\n",
      "Epoch 157/500\n",
      "Train loss 0.40855263730129565 accuracy 0.7367624640464783\n",
      "Val loss 2.249688756465912 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 158/500\n",
      "Train loss 0.4079603692853307 accuracy 0.7399773001670837\n",
      "Val loss 1.428366732597351 accuracy 0.6870748400688171\n",
      "----------\n",
      "Epoch 159/500\n",
      "Train loss 0.4060181968183403 accuracy 0.7369515895843506\n",
      "Val loss 1.7504024744033813 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 160/500\n",
      "Train loss 0.4014004971607622 accuracy 0.7414901256561279\n",
      "Val loss 2.3529592990875243 accuracy 0.6879251599311829\n",
      "----------\n",
      "Epoch 161/500\n",
      "Train loss 0.4023569815130119 accuracy 0.7397881746292114\n",
      "Val loss 2.441820442676544 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 162/500\n",
      "Train loss 0.4086251104452524 accuracy 0.7372352480888367\n",
      "Val loss 2.01590256690979 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 163/500\n",
      "Train loss 0.41126669925379467 accuracy 0.7346822619438171\n",
      "Val loss 1.4621103048324584 accuracy 0.6896258592605591\n",
      "----------\n",
      "Epoch 164/500\n",
      "Train loss 0.40537796393934505 accuracy 0.7342094779014587\n",
      "Val loss 2.374965453147888 accuracy 0.6879251599311829\n",
      "----------\n",
      "Epoch 165/500\n",
      "Train loss 0.4091737403208951 accuracy 0.7353441715240479\n",
      "Val loss 2.612389552593231 accuracy 0.6845238208770752\n",
      "----------\n",
      "Epoch 166/500\n",
      "Train loss 0.41355807989476673 accuracy 0.7363842129707336\n",
      "Val loss 2.5506577491760254 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 167/500\n",
      "Train loss 0.4143824990255287 accuracy 0.733453094959259\n",
      "Val loss 1.8801389813423157 accuracy 0.6930271983146667\n",
      "----------\n",
      "Epoch 168/500\n",
      "Train loss 0.4108832349260169 accuracy 0.7346822619438171\n",
      "Val loss 2.137165975570679 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 169/500\n",
      "Train loss 0.4038341206958495 accuracy 0.7444213032722473\n",
      "Val loss 2.8569773495197297 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 170/500\n",
      "Train loss 0.41164898261966476 accuracy 0.7314674258232117\n",
      "Val loss 2.315704905986786 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 171/500\n",
      "Train loss 0.412870260247265 accuracy 0.7350605130195618\n",
      "Val loss 2.0761221051216125 accuracy 0.6726190447807312\n",
      "----------\n",
      "Epoch 172/500\n",
      "Train loss 0.4114155977605337 accuracy 0.7372352480888367\n",
      "Val loss 2.7125508308410646 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 173/500\n",
      "Train loss 0.4066378905112485 accuracy 0.7335476279258728\n",
      "Val loss 5.10457124710083 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 174/500\n",
      "Train loss 0.41569913546723053 accuracy 0.7345877289772034\n",
      "Val loss 2.888542950153351 accuracy 0.692176878452301\n",
      "----------\n",
      "Epoch 175/500\n",
      "Train loss 0.4094073090208582 accuracy 0.7351550459861755\n",
      "Val loss 2.7864670157432556 accuracy 0.6896258592605591\n",
      "----------\n",
      "Epoch 176/500\n",
      "Train loss 0.41490308168422746 accuracy 0.7351550459861755\n",
      "Val loss 3.301639175415039 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 177/500\n",
      "Train loss 0.4124020557087588 accuracy 0.733453094959259\n",
      "Val loss 2.4313318610191343 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 178/500\n",
      "Train loss 0.4148378677396889 accuracy 0.7362896800041199\n",
      "Val loss 2.1935160160064697 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 179/500\n",
      "Train loss 0.4064874982977488 accuracy 0.7414901256561279\n",
      "Val loss 2.718854522705078 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 180/500\n",
      "Train loss 0.4114884507943349 accuracy 0.7326966524124146\n",
      "Val loss 2.733626997470856 accuracy 0.6862244606018066\n",
      "----------\n",
      "Epoch 181/500\n",
      "Train loss 0.4087170664086399 accuracy 0.7367624640464783\n",
      "Val loss 2.3750425219535827 accuracy 0.6879251599311829\n",
      "----------\n",
      "Epoch 182/500\n",
      "Train loss 0.4077489444290299 accuracy 0.7380862236022949\n",
      "Val loss 3.033249020576477 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 183/500\n",
      "Train loss 0.40715464339198837 accuracy 0.7367624640464783\n",
      "Val loss 3.0682358026504515 accuracy 0.6938775181770325\n",
      "----------\n",
      "Epoch 184/500\n",
      "Train loss 0.40364399695970926 accuracy 0.7397881746292114\n",
      "Val loss 3.076533854007721 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 185/500\n",
      "Train loss 0.40010671407343396 accuracy 0.7412064671516418\n",
      "Val loss 3.010468769073486 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 186/500\n",
      "Train loss 0.4003273833947009 accuracy 0.7415847182273865\n",
      "Val loss 2.9171895742416383 accuracy 0.6938775181770325\n",
      "----------\n",
      "Epoch 187/500\n",
      "Train loss 0.4038871630846736 accuracy 0.7392208576202393\n",
      "Val loss 2.844203197956085 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 188/500\n",
      "Train loss 0.40193718276828166 accuracy 0.7405446171760559\n",
      "Val loss 2.842773401737213 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 189/500\n",
      "Train loss 0.40952061637338383 accuracy 0.7346822619438171\n",
      "Val loss 3.023581802845001 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 190/500\n",
      "Train loss 0.40409016214221355 accuracy 0.7362896800041199\n",
      "Val loss 3.231840705871582 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 191/500\n",
      "Train loss 0.4042798259172095 accuracy 0.741773784160614\n",
      "Val loss 3.3983471989631653 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 192/500\n",
      "Train loss 0.4088659789188799 accuracy 0.7348713874816895\n",
      "Val loss 2.203523504734039 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 193/500\n",
      "Train loss 0.40778373522930833 accuracy 0.740828275680542\n",
      "Val loss 2.5942275166511535 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 194/500\n",
      "Train loss 0.4009869680347213 accuracy 0.7430030107498169\n",
      "Val loss 2.943553650379181 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 195/500\n",
      "Train loss 0.41196138456643344 accuracy 0.7345877289772034\n",
      "Val loss 3.246224069595337 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 196/500\n",
      "Train loss 0.40468377557145546 accuracy 0.7386535406112671\n",
      "Val loss 3.3852494239807127 accuracy 0.6879251599311829\n",
      "----------\n",
      "Epoch 197/500\n",
      "Train loss 0.4037045804132898 accuracy 0.739315390586853\n",
      "Val loss 3.9912365674972534 accuracy 0.6964285373687744\n",
      "----------\n",
      "Epoch 198/500\n",
      "Train loss 0.41624058585569085 accuracy 0.7337367534637451\n",
      "Val loss 2.2537676811218263 accuracy 0.6930271983146667\n",
      "----------\n",
      "Epoch 199/500\n",
      "Train loss 0.40670461252511264 accuracy 0.7376134395599365\n",
      "Val loss 2.982939100265503 accuracy 0.6777210831642151\n",
      "----------\n",
      "Epoch 200/500\n",
      "Train loss 0.40480111234159355 accuracy 0.739315390586853\n",
      "Val loss 2.3193576335906982 accuracy 0.6964285373687744\n",
      "----------\n",
      "Epoch 201/500\n",
      "Train loss 0.40853925426322296 accuracy 0.736573338508606\n",
      "Val loss 2.8775283813476564 accuracy 0.692176878452301\n",
      "----------\n",
      "Epoch 202/500\n",
      "Train loss 0.40970709094082014 accuracy 0.7321293354034424\n",
      "Val loss 2.5281649351119997 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 203/500\n",
      "Train loss 0.40742307685943974 accuracy 0.7390317320823669\n",
      "Val loss 3.131528449058533 accuracy 0.7049319744110107\n",
      "----------\n",
      "Epoch 204/500\n",
      "Train loss 0.40310615337038613 accuracy 0.7416792511940002\n",
      "Val loss 2.355523669719696 accuracy 0.6930271983146667\n",
      "----------\n",
      "Epoch 205/500\n",
      "Train loss 0.4114622617342386 accuracy 0.7325075268745422\n",
      "Val loss 2.8022452712059023 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 206/500\n",
      "Train loss 0.41264232167278425 accuracy 0.7333585023880005\n",
      "Val loss 3.848138451576233 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 207/500\n",
      "Train loss 0.40743525416018017 accuracy 0.7405446171760559\n",
      "Val loss 2.79952986240387 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 208/500\n",
      "Train loss 0.4052176019513463 accuracy 0.7405446171760559\n",
      "Val loss 2.4928425669670107 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 209/500\n",
      "Train loss 0.41309122436017875 accuracy 0.7332639694213867\n",
      "Val loss 1.7312346339225768 accuracy 0.6904761791229248\n",
      "----------\n",
      "Epoch 210/500\n",
      "Train loss 0.4127935580460422 accuracy 0.7349659204483032\n",
      "Val loss 2.1064738750457765 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 211/500\n",
      "Train loss 0.40507288869605007 accuracy 0.740828275680542\n",
      "Val loss 2.34384970664978 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 212/500\n",
      "Train loss 0.4111974433243993 accuracy 0.7351550459861755\n",
      "Val loss 3.021515965461731 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 213/500\n",
      "Train loss 0.4107104582958911 accuracy 0.7395990490913391\n",
      "Val loss 2.960905933380127 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 214/500\n",
      "Train loss 0.40921295515025957 accuracy 0.7374243140220642\n",
      "Val loss 2.202391791343689 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 215/500\n",
      "Train loss 0.4021449803587902 accuracy 0.7395990490913391\n",
      "Val loss 2.4946985602378846 accuracy 0.6981292366981506\n",
      "----------\n",
      "Epoch 216/500\n",
      "Train loss 0.3977214162608227 accuracy 0.7451777458190918\n",
      "Val loss 2.2043254375457764 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 217/500\n",
      "Train loss 0.4060426627296999 accuracy 0.7421520352363586\n",
      "Val loss 2.4935230135917665 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 218/500\n",
      "Train loss 0.408034292689289 accuracy 0.7347768545150757\n",
      "Val loss 2.8985782861709595 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 219/500\n",
      "Train loss 0.4043260313660266 accuracy 0.7377079725265503\n",
      "Val loss 3.320712387561798 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 220/500\n",
      "Train loss 0.41235968003790063 accuracy 0.7346822619438171\n",
      "Val loss 3.4615330934524535 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 221/500\n",
      "Train loss 0.4086575561977295 accuracy 0.7379916310310364\n",
      "Val loss 3.1499751567840577 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 222/500\n",
      "Train loss 0.4092369244759341 accuracy 0.7377079725265503\n",
      "Val loss 1.6084349870681762 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 223/500\n",
      "Train loss 0.4115463243909629 accuracy 0.7322238683700562\n",
      "Val loss 2.5200660943984987 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 224/500\n",
      "Train loss 0.40648753061352005 accuracy 0.7375189065933228\n",
      "Val loss 3.1087815046310423 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 225/500\n",
      "Train loss 0.4127099682767707 accuracy 0.7318456768989563\n",
      "Val loss 2.6472986936569214 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 226/500\n",
      "Train loss 0.4055526579718992 accuracy 0.738369882106781\n",
      "Val loss 2.5165834188461305 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 227/500\n",
      "Train loss 0.40687619347170173 accuracy 0.741773784160614\n",
      "Val loss 2.88199742436409 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 228/500\n",
      "Train loss 0.3983300019459552 accuracy 0.7411119341850281\n",
      "Val loss 3.332041358947754 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 229/500\n",
      "Train loss 0.40957952622907706 accuracy 0.7345877289772034\n",
      "Val loss 2.8217106103897094 accuracy 0.714285671710968\n",
      "----------\n",
      "Epoch 230/500\n",
      "Train loss 0.4049408119845103 accuracy 0.7382752895355225\n",
      "Val loss 3.6020817518234254 accuracy 0.7040815949440002\n",
      "----------\n",
      "Epoch 231/500\n",
      "Train loss 0.4111393345407693 accuracy 0.7343040704727173\n",
      "Val loss 4.117821264266968 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 232/500\n",
      "Train loss 0.4037426957883031 accuracy 0.7388426065444946\n",
      "Val loss 4.542541742324829 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 233/500\n",
      "Train loss 0.40891676674406213 accuracy 0.7366678714752197\n",
      "Val loss 3.9723276853561402 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 234/500\n",
      "Train loss 0.41158552342150584 accuracy 0.7315620183944702\n",
      "Val loss 3.360015606880188 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 235/500\n",
      "Train loss 0.41139391710959283 accuracy 0.7351550459861755\n",
      "Val loss 3.4289407253265383 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 236/500\n",
      "Train loss 0.4052945633968675 accuracy 0.7378025650978088\n",
      "Val loss 3.7096688866615297 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 237/500\n",
      "Train loss 0.4056823943034712 accuracy 0.736856997013092\n",
      "Val loss 4.292283594608307 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 238/500\n",
      "Train loss 0.4045944027153842 accuracy 0.7351550459861755\n",
      "Val loss 5.190592801570892 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 239/500\n",
      "Train loss 0.40947540337780874 accuracy 0.733169436454773\n",
      "Val loss 4.248891353607178 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 240/500\n",
      "Train loss 0.39921728207404356 accuracy 0.7433812022209167\n",
      "Val loss 5.624422597885132 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 241/500\n",
      "Train loss 0.41010416810771066 accuracy 0.7370461225509644\n",
      "Val loss 2.759373962879181 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 242/500\n",
      "Train loss 0.4049886813364833 accuracy 0.7395990490913391\n",
      "Val loss 3.351113295555115 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 243/500\n",
      "Train loss 0.41404423512608174 accuracy 0.7330748438835144\n",
      "Val loss 3.0872772455215456 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 244/500\n",
      "Train loss 0.41335670883397024 accuracy 0.7327911853790283\n",
      "Val loss 3.361261582374573 accuracy 0.6887754797935486\n",
      "----------\n",
      "Epoch 245/500\n",
      "Train loss 0.41235835114157343 accuracy 0.7352495789527893\n",
      "Val loss 3.5379888296127318 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 246/500\n",
      "Train loss 0.4098773763840457 accuracy 0.7392208576202393\n",
      "Val loss 4.244600987434387 accuracy 0.6964285373687744\n",
      "----------\n",
      "Epoch 247/500\n",
      "Train loss 0.4052760697991015 accuracy 0.7398827075958252\n",
      "Val loss 3.810360836982727 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 248/500\n",
      "Train loss 0.40917809792311793 accuracy 0.7353441715240479\n",
      "Val loss 3.5876027822494505 accuracy 0.7032312750816345\n",
      "----------\n",
      "Epoch 249/500\n",
      "Train loss 0.4082455383725913 accuracy 0.7322238683700562\n",
      "Val loss 3.8058918118476868 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 250/500\n",
      "Train loss 0.41175584548927213 accuracy 0.7332639694213867\n",
      "Val loss 3.9373606204986573 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 251/500\n",
      "Train loss 0.4046562353530562 accuracy 0.739315390586853\n",
      "Val loss 4.204578161239624 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 252/500\n",
      "Train loss 0.40761715245534136 accuracy 0.7363842129707336\n",
      "Val loss 4.477754521369934 accuracy 0.6964285373687744\n",
      "----------\n",
      "Epoch 253/500\n",
      "Train loss 0.41097480812704706 accuracy 0.7294818162918091\n",
      "Val loss 5.701222383975983 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 254/500\n",
      "Train loss 0.4109199477965573 accuracy 0.7369515895843506\n",
      "Val loss 3.5379094004631044 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 255/500\n",
      "Train loss 0.40410607119640674 accuracy 0.7372352480888367\n",
      "Val loss 4.784254646301269 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 256/500\n",
      "Train loss 0.4130670270287847 accuracy 0.7347768545150757\n",
      "Val loss 1.8951440572738647 accuracy 0.714285671710968\n",
      "----------\n",
      "Epoch 257/500\n",
      "Train loss 0.4153216369898923 accuracy 0.7320347428321838\n",
      "Val loss 2.594858080148697 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 258/500\n",
      "Train loss 0.40211342616253587 accuracy 0.7400718331336975\n",
      "Val loss 2.5824615597724914 accuracy 0.6904761791229248\n",
      "----------\n",
      "Epoch 259/500\n",
      "Train loss 0.40814846035945845 accuracy 0.7360060214996338\n",
      "Val loss 3.169253444671631 accuracy 0.6913264989852905\n",
      "----------\n",
      "Epoch 260/500\n",
      "Train loss 0.4044345426990325 accuracy 0.7397881746292114\n",
      "Val loss 3.086423373222351 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 261/500\n",
      "Train loss 0.4066015948732215 accuracy 0.7375189065933228\n",
      "Val loss 3.1873990535736083 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 262/500\n",
      "Train loss 0.40903501015111626 accuracy 0.73591148853302\n",
      "Val loss 3.4748641729354857 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 263/500\n",
      "Train loss 0.40265017101563605 accuracy 0.7360060214996338\n",
      "Val loss 3.704823637008667 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 264/500\n",
      "Train loss 0.4043209100344095 accuracy 0.7394099831581116\n",
      "Val loss 3.6589848399162292 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 265/500\n",
      "Train loss 0.40554489141487216 accuracy 0.7399773001670837\n",
      "Val loss 3.7185359477996824 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 266/500\n",
      "Train loss 0.4059858584260366 accuracy 0.7363842129707336\n",
      "Val loss 3.8194342613220216 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 267/500\n",
      "Train loss 0.4064782276211015 accuracy 0.7394099831581116\n",
      "Val loss 4.144572925567627 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 268/500\n",
      "Train loss 0.40048892095864536 accuracy 0.740828275680542\n",
      "Val loss 3.808713912963867 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 269/500\n",
      "Train loss 0.40409348815320484 accuracy 0.7404500246047974\n",
      "Val loss 3.747785413265228 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 270/500\n",
      "Train loss 0.4070293598146324 accuracy 0.7384644150733948\n",
      "Val loss 3.957413077354431 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 271/500\n",
      "Train loss 0.4072527946477913 accuracy 0.7356278300285339\n",
      "Val loss 3.891170525550842 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 272/500\n",
      "Train loss 0.4000228162271431 accuracy 0.7423411011695862\n",
      "Val loss 3.779206657409668 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 273/500\n",
      "Train loss 0.40440026284700414 accuracy 0.7379916310310364\n",
      "Val loss 4.127333450317383 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 274/500\n",
      "Train loss 0.41070283105574457 accuracy 0.7321293354034424\n",
      "Val loss 4.318972229957581 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 275/500\n",
      "Train loss 0.40234292271625566 accuracy 0.740828275680542\n",
      "Val loss 4.085920000076294 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 276/500\n",
      "Train loss 0.404282765216138 accuracy 0.7352495789527893\n",
      "Val loss 4.310499215126038 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 277/500\n",
      "Train loss 0.4051001887005496 accuracy 0.7361951470375061\n",
      "Val loss 4.93888247013092 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 278/500\n",
      "Train loss 0.41081096932112454 accuracy 0.7357223629951477\n",
      "Val loss 4.005890822410583 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 279/500\n",
      "Train loss 0.4069293521973024 accuracy 0.7361951470375061\n",
      "Val loss 4.251699614524841 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 280/500\n",
      "Train loss 0.40821255546018304 accuracy 0.736856997013092\n",
      "Val loss 4.209980702400207 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 281/500\n",
      "Train loss 0.397362040108945 accuracy 0.740828275680542\n",
      "Val loss 4.2078423500061035 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 282/500\n",
      "Train loss 0.4010102842227522 accuracy 0.7389371991157532\n",
      "Val loss 3.98405442237854 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 283/500\n",
      "Train loss 0.40218686770243817 accuracy 0.7376134395599365\n",
      "Val loss 4.067099642753601 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 284/500\n",
      "Train loss 0.40195147028888567 accuracy 0.7420574426651001\n",
      "Val loss 4.17486605644226 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 285/500\n",
      "Train loss 0.4074079433837569 accuracy 0.7339258193969727\n",
      "Val loss 4.3893828868865965 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 286/500\n",
      "Train loss 0.40262457477041036 accuracy 0.7353441715240479\n",
      "Val loss 4.527186536788941 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 287/500\n",
      "Train loss 0.40458603077624217 accuracy 0.7394099831581116\n",
      "Val loss 4.206318736076355 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 288/500\n",
      "Train loss 0.41011256159070025 accuracy 0.7355332374572754\n",
      "Val loss 4.350070786476135 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 289/500\n",
      "Train loss 0.4049274433090026 accuracy 0.7377079725265503\n",
      "Val loss 4.3260822057723995 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 290/500\n",
      "Train loss 0.40545731064784957 accuracy 0.7386535406112671\n",
      "Val loss 4.274741911888123 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 291/500\n",
      "Train loss 0.4060051854116371 accuracy 0.7354387044906616\n",
      "Val loss 4.564533114433289 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 292/500\n",
      "Train loss 0.4042554756244981 accuracy 0.7407336831092834\n",
      "Val loss 4.050028026103973 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 293/500\n",
      "Train loss 0.40809632963444814 accuracy 0.7344931364059448\n",
      "Val loss 4.161814785003662 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 294/500\n",
      "Train loss 0.40957892623292397 accuracy 0.7348713874816895\n",
      "Val loss 4.315323162078857 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 295/500\n",
      "Train loss 0.4097369898514575 accuracy 0.7367624640464783\n",
      "Val loss 4.640277671813965 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 296/500\n",
      "Train loss 0.40515529929873456 accuracy 0.7382752895355225\n",
      "Val loss 4.367950057983398 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 297/500\n",
      "Train loss 0.40144068684922646 accuracy 0.7427193522453308\n",
      "Val loss 4.431523084640503 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 298/500\n",
      "Train loss 0.4018319529223155 accuracy 0.7389371991157532\n",
      "Val loss 4.53470253944397 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 299/500\n",
      "Train loss 0.4120142212115138 accuracy 0.7363842129707336\n",
      "Val loss 4.582243609428406 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 300/500\n",
      "Train loss 0.40664551882858735 accuracy 0.7378970980644226\n",
      "Val loss 4.5083530902862545 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 301/500\n",
      "Train loss 0.40040515632514495 accuracy 0.7439485192298889\n",
      "Val loss 4.400681757926941 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 302/500\n",
      "Train loss 0.4052280036799879 accuracy 0.7373297810554504\n",
      "Val loss 4.435586929321289 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 303/500\n",
      "Train loss 0.40215166182403106 accuracy 0.7401663661003113\n",
      "Val loss 4.874361181259156 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 304/500\n",
      "Train loss 0.4031425359019314 accuracy 0.7399773001670837\n",
      "Val loss 4.922623324394226 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 305/500\n",
      "Train loss 0.40725397596876306 accuracy 0.7381807565689087\n",
      "Val loss 4.619418644905091 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 306/500\n",
      "Train loss 0.4002942672695022 accuracy 0.7403554916381836\n",
      "Val loss 4.64389181137085 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 307/500\n",
      "Train loss 0.4067889627922012 accuracy 0.7372352480888367\n",
      "Val loss 4.921459746360779 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 308/500\n",
      "Train loss 0.40564520064606724 accuracy 0.7369515895843506\n",
      "Val loss 4.588682019710541 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 309/500\n",
      "Train loss 0.3995567578149129 accuracy 0.7447994947433472\n",
      "Val loss 4.635003006458282 accuracy 0.7032312750816345\n",
      "----------\n",
      "Epoch 310/500\n",
      "Train loss 0.4048709653946291 accuracy 0.7388426065444946\n",
      "Val loss 4.683511745929718 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 311/500\n",
      "Train loss 0.40522357606026066 accuracy 0.7370461225509644\n",
      "Val loss 4.9997505187988285 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 312/500\n",
      "Train loss 0.4111456788447966 accuracy 0.7332639694213867\n",
      "Val loss 5.123291015625 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 313/500\n",
      "Train loss 0.4068821726075138 accuracy 0.7367624640464783\n",
      "Val loss 5.394263458251953 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 314/500\n",
      "Train loss 0.40397680236632566 accuracy 0.7401663661003113\n",
      "Val loss 5.039094591140747 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 315/500\n",
      "Train loss 0.4052124996501279 accuracy 0.7379916310310364\n",
      "Val loss 5.256197619438171 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 316/500\n",
      "Train loss 0.4017962375319148 accuracy 0.7388426065444946\n",
      "Val loss 5.0299814462661745 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 317/500\n",
      "Train loss 0.40190205516585387 accuracy 0.7400718331336975\n",
      "Val loss 5.05472629070282 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 318/500\n",
      "Train loss 0.4123289147055293 accuracy 0.7373297810554504\n",
      "Val loss 4.943420803546905 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 319/500\n",
      "Train loss 0.40862128425793476 accuracy 0.7379916310310364\n",
      "Val loss 5.36053056716919 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 320/500\n",
      "Train loss 0.41026920223810587 accuracy 0.7375189065933228\n",
      "Val loss 5.199688744544983 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 321/500\n",
      "Train loss 0.404033956757511 accuracy 0.738369882106781\n",
      "Val loss 5.016669493913651 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 322/500\n",
      "Train loss 0.412404859281448 accuracy 0.7358168959617615\n",
      "Val loss 6.338696837425232 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 323/500\n",
      "Train loss 0.4236762268715594 accuracy 0.733169436454773\n",
      "Val loss 2.445689082145691 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 324/500\n",
      "Train loss 0.42428323411079777 accuracy 0.7330748438835144\n",
      "Val loss 3.3675227880477907 accuracy 0.7049319744110107\n",
      "----------\n",
      "Epoch 325/500\n",
      "Train loss 0.4211808730320758 accuracy 0.7298600077629089\n",
      "Val loss 5.213959431648254 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 326/500\n",
      "Train loss 0.4126719484128148 accuracy 0.7319402098655701\n",
      "Val loss 6.809980928897858 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 327/500\n",
      "Train loss 0.4166067695043173 accuracy 0.7370461225509644\n",
      "Val loss 2.038105070590973 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 328/500\n",
      "Train loss 0.4113138127757842 accuracy 0.7354387044906616\n",
      "Val loss 1.741102123260498 accuracy 0.6972789168357849\n",
      "----------\n",
      "Epoch 329/500\n",
      "Train loss 0.40244137486779546 accuracy 0.7441376447677612\n",
      "Val loss 3.2706870555877687 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 330/500\n",
      "Train loss 0.41023661034652986 accuracy 0.7346822619438171\n",
      "Val loss 3.5704213619232177 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 331/500\n",
      "Train loss 0.40752227442810335 accuracy 0.7386535406112671\n",
      "Val loss 4.519831347465515 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 332/500\n",
      "Train loss 0.4119190950709653 accuracy 0.7358168959617615\n",
      "Val loss 3.8560728430747986 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 333/500\n",
      "Train loss 0.406059039285384 accuracy 0.7376134395599365\n",
      "Val loss 4.949376916885376 accuracy 0.6896258592605591\n",
      "----------\n",
      "Epoch 334/500\n",
      "Train loss 0.4080537650958601 accuracy 0.7361005544662476\n",
      "Val loss 7.578843140602112 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 335/500\n",
      "Train loss 0.4095725147839052 accuracy 0.7362896800041199\n",
      "Val loss 2.11082866191864 accuracy 0.6964285373687744\n",
      "----------\n",
      "Epoch 336/500\n",
      "Train loss 0.4051342653222831 accuracy 0.7333585023880005\n",
      "Val loss 7.075551462173462 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 337/500\n",
      "Train loss 0.40481362177664976 accuracy 0.7397881746292114\n",
      "Val loss 3.9556193709373475 accuracy 0.714285671710968\n",
      "----------\n",
      "Epoch 338/500\n",
      "Train loss 0.40048505060644035 accuracy 0.7435703277587891\n",
      "Val loss 6.548450565338134 accuracy 0.7151360511779785\n",
      "----------\n",
      "Epoch 339/500\n",
      "Train loss 0.4125002209680626 accuracy 0.7294818162918091\n",
      "Val loss 5.638954305648804 accuracy 0.7151360511779785\n",
      "----------\n",
      "Epoch 340/500\n",
      "Train loss 0.40413730582558965 accuracy 0.7378025650978088\n",
      "Val loss 5.809362053871155 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 341/500\n",
      "Train loss 0.4053710688309497 accuracy 0.7395990490913391\n",
      "Val loss 8.82752194404602 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 342/500\n",
      "Train loss 0.4052154920905469 accuracy 0.7381807565689087\n",
      "Val loss 5.680478584766388 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 343/500\n",
      "Train loss 0.4051863831209849 accuracy 0.7360060214996338\n",
      "Val loss 6.18566010594368 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 344/500\n",
      "Train loss 0.40994885994727354 accuracy 0.7361005544662476\n",
      "Val loss 7.3841509342193605 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 345/500\n",
      "Train loss 0.40807019587022714 accuracy 0.7358168959617615\n",
      "Val loss 6.596546030044555 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 346/500\n",
      "Train loss 0.40790915022413415 accuracy 0.7345877289772034\n",
      "Val loss 7.174782609939575 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 347/500\n",
      "Train loss 0.4047008210636047 accuracy 0.7407336831092834\n",
      "Val loss 7.395119524002075 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 348/500\n",
      "Train loss 0.40329216259071626 accuracy 0.7399773001670837\n",
      "Val loss 7.252265000343323 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 349/500\n",
      "Train loss 0.39911575812891303 accuracy 0.7441376447677612\n",
      "Val loss 7.056255435943603 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 350/500\n",
      "Train loss 0.4064916655959853 accuracy 0.7347768545150757\n",
      "Val loss 6.650282603502274 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 351/500\n",
      "Train loss 0.4024497062326914 accuracy 0.7411119341850281\n",
      "Val loss 7.682452154159546 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 352/500\n",
      "Train loss 0.4046456917940852 accuracy 0.7407336831092834\n",
      "Val loss 7.3324686050415036 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 353/500\n",
      "Train loss 0.4042729968766132 accuracy 0.7387480735778809\n",
      "Val loss 7.346161675453186 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 354/500\n",
      "Train loss 0.4094223656568183 accuracy 0.7370461225509644\n",
      "Val loss 8.528280687332153 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 355/500\n",
      "Train loss 0.40426983711231185 accuracy 0.7401663661003113\n",
      "Val loss 7.24095470905304 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 356/500\n",
      "Train loss 0.4040931736130312 accuracy 0.7398827075958252\n",
      "Val loss 7.709101724624634 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 357/500\n",
      "Train loss 0.40203410602477657 accuracy 0.7401663661003113\n",
      "Val loss 8.324620962142944 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 358/500\n",
      "Train loss 0.40782462904252204 accuracy 0.7355332374572754\n",
      "Val loss 7.378327417373657 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 359/500\n",
      "Train loss 0.4068395926291684 accuracy 0.7385589480400085\n",
      "Val loss 7.882715177536011 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 360/500\n",
      "Train loss 0.4074897277786071 accuracy 0.7360060214996338\n",
      "Val loss 7.497751021385193 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 361/500\n",
      "Train loss 0.3997254177748439 accuracy 0.740828275680542\n",
      "Val loss 7.916420984268188 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 362/500\n",
      "Train loss 0.41447991240455445 accuracy 0.7317510843276978\n",
      "Val loss 7.944743633270264 accuracy 0.714285671710968\n",
      "----------\n",
      "Epoch 363/500\n",
      "Train loss 0.4104856148541692 accuracy 0.7376134395599365\n",
      "Val loss 2.897072601318359 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 364/500\n",
      "Train loss 0.4174708842513073 accuracy 0.7300491333007812\n",
      "Val loss 4.938155651092529 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 365/500\n",
      "Train loss 0.4123840012464179 accuracy 0.73591148853302\n",
      "Val loss 5.381718635559082 accuracy 0.6896258592605591\n",
      "----------\n",
      "Epoch 366/500\n",
      "Train loss 0.4115259011825883 accuracy 0.7357223629951477\n",
      "Val loss 4.650839138031006 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 367/500\n",
      "Train loss 0.41995445863310116 accuracy 0.7343040704727173\n",
      "Val loss 4.483411872386933 accuracy 0.692176878452301\n",
      "----------\n",
      "Epoch 368/500\n",
      "Train loss 0.410036827785423 accuracy 0.7323184013366699\n",
      "Val loss 4.338889765739441 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 369/500\n",
      "Train loss 0.4107870287205799 accuracy 0.7348713874816895\n",
      "Val loss 6.5487510681152346 accuracy 0.692176878452301\n",
      "----------\n",
      "Epoch 370/500\n",
      "Train loss 0.406529323164239 accuracy 0.7415847182273865\n",
      "Val loss 5.337158179283142 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 371/500\n",
      "Train loss 0.4064401693372841 accuracy 0.7369515895843506\n",
      "Val loss 6.644144582748413 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 372/500\n",
      "Train loss 0.4075028311057263 accuracy 0.7366678714752197\n",
      "Val loss 6.551796960830688 accuracy 0.6930271983146667\n",
      "----------\n",
      "Epoch 373/500\n",
      "Train loss 0.4059282491006047 accuracy 0.7371406555175781\n",
      "Val loss 7.100915908813477 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 374/500\n",
      "Train loss 0.40362869975078536 accuracy 0.7374243140220642\n",
      "Val loss 8.134196949005126 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 375/500\n",
      "Train loss 0.4100965067564723 accuracy 0.7378970980644226\n",
      "Val loss 4.221007394790649 accuracy 0.7006802558898926\n",
      "----------\n",
      "Epoch 376/500\n",
      "Train loss 0.41071138683571873 accuracy 0.7319402098655701\n",
      "Val loss 5.231186866760254 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 377/500\n",
      "Train loss 0.4062830443123737 accuracy 0.7419629096984863\n",
      "Val loss 5.2470509052276615 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 378/500\n",
      "Train loss 0.4030394970652569 accuracy 0.743286669254303\n",
      "Val loss 4.2585105180740355 accuracy 0.6955782175064087\n",
      "----------\n",
      "Epoch 379/500\n",
      "Train loss 0.40890598656183264 accuracy 0.7378025650978088\n",
      "Val loss 5.372688889503479 accuracy 0.6998299360275269\n",
      "----------\n",
      "Epoch 380/500\n",
      "Train loss 0.4147580564022064 accuracy 0.7343040704727173\n",
      "Val loss 5.746567940711975 accuracy 0.7015305757522583\n",
      "----------\n",
      "Epoch 381/500\n",
      "Train loss 0.40952831470822715 accuracy 0.7362896800041199\n",
      "Val loss 5.766524314880371 accuracy 0.6938775181770325\n",
      "----------\n",
      "Epoch 382/500\n",
      "Train loss 0.4116282398442188 accuracy 0.7385589480400085\n",
      "Val loss 4.806266677379608 accuracy 0.694727897644043\n",
      "----------\n",
      "Epoch 383/500\n",
      "Train loss 0.40751061920660087 accuracy 0.7398827075958252\n",
      "Val loss 6.52393000125885 accuracy 0.6853741407394409\n",
      "----------\n",
      "Epoch 384/500\n",
      "Train loss 0.4152002754699753 accuracy 0.7347768545150757\n",
      "Val loss 3.528285467624664 accuracy 0.7117346525192261\n",
      "----------\n",
      "Epoch 385/500\n",
      "Train loss 0.4106843888041485 accuracy 0.7322238683700562\n",
      "Val loss 2.72635737657547 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 386/500\n",
      "Train loss 0.41020758288452425 accuracy 0.7357223629951477\n",
      "Val loss 3.3507211446762084 accuracy 0.7023809552192688\n",
      "----------\n",
      "Epoch 387/500\n",
      "Train loss 0.40778704544147815 accuracy 0.7347768545150757\n",
      "Val loss 3.0120236754417418 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 388/500\n",
      "Train loss 0.4061724141419652 accuracy 0.738369882106781\n",
      "Val loss 3.272623324394226 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 389/500\n",
      "Train loss 0.4090431677289756 accuracy 0.7401663661003113\n",
      "Val loss 3.629341924190521 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 390/500\n",
      "Train loss 0.40275748917855414 accuracy 0.7354387044906616\n",
      "Val loss 4.868668532371521 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 391/500\n",
      "Train loss 0.40776988074003934 accuracy 0.7395990490913391\n",
      "Val loss 4.316496014595032 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 392/500\n",
      "Train loss 0.4103936987468995 accuracy 0.7371406555175781\n",
      "Val loss 9.02917275428772 accuracy 0.6989795565605164\n",
      "----------\n",
      "Epoch 393/500\n",
      "Train loss 0.40690742630556404 accuracy 0.7411119341850281\n",
      "Val loss 3.5249644994735716 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 394/500\n",
      "Train loss 0.40457423372441026 accuracy 0.7372352480888367\n",
      "Val loss 3.0211962938308714 accuracy 0.7074829936027527\n",
      "----------\n",
      "Epoch 395/500\n",
      "Train loss 0.4039593726755625 accuracy 0.7370461225509644\n",
      "Val loss 3.3698008060455322 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 396/500\n",
      "Train loss 0.4129907413419471 accuracy 0.7314674258232117\n",
      "Val loss 3.5218674302101136 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 397/500\n",
      "Train loss 0.4062210737940777 accuracy 0.736573338508606\n",
      "Val loss 3.885411524772644 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 398/500\n",
      "Train loss 0.4030192720602794 accuracy 0.7378025650978088\n",
      "Val loss 4.33126392364502 accuracy 0.7083333134651184\n",
      "----------\n",
      "Epoch 399/500\n",
      "Train loss 0.41234326362609863 accuracy 0.7353441715240479\n",
      "Val loss 4.358974599838257 accuracy 0.7057822942733765\n",
      "----------\n",
      "Epoch 400/500\n",
      "Train loss 0.4130254140101283 accuracy 0.7309001088142395\n",
      "Val loss 3.55389347076416 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 401/500\n",
      "Train loss 0.4120467452399702 accuracy 0.7342094779014587\n",
      "Val loss 4.216588926315308 accuracy 0.7066326141357422\n",
      "----------\n",
      "Epoch 402/500\n",
      "Train loss 0.4079029086124466 accuracy 0.7304273843765259\n",
      "Val loss 3.909858775138855 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 403/500\n",
      "Train loss 0.4069995567741164 accuracy 0.7390317320823669\n",
      "Val loss 4.316807556152344 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 404/500\n",
      "Train loss 0.40556837313146477 accuracy 0.7397881746292114\n",
      "Val loss 4.0092637807130815 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 405/500\n",
      "Train loss 0.4079292847449521 accuracy 0.7366678714752197\n",
      "Val loss 4.33457133769989 accuracy 0.7117346525192261\n",
      "----------\n",
      "Epoch 406/500\n",
      "Train loss 0.405048694237169 accuracy 0.7395990490913391\n",
      "Val loss 4.1608252167701725 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 407/500\n",
      "Train loss 0.4021796077848917 accuracy 0.7384644150733948\n",
      "Val loss 4.304748129844666 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 408/500\n",
      "Train loss 0.4092239066060767 accuracy 0.7366678714752197\n",
      "Val loss 4.335140895843506 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 409/500\n",
      "Train loss 0.4049511770886111 accuracy 0.7394099831581116\n",
      "Val loss 4.461405992507935 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 410/500\n",
      "Train loss 0.40107263642621327 accuracy 0.7402609586715698\n",
      "Val loss 4.850311803817749 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 411/500\n",
      "Train loss 0.4119383667606905 accuracy 0.7338312864303589\n",
      "Val loss 4.275086712837219 accuracy 0.7117346525192261\n",
      "----------\n",
      "Epoch 412/500\n",
      "Train loss 0.40219753262508345 accuracy 0.7397881746292114\n",
      "Val loss 4.536357951164246 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 413/500\n",
      "Train loss 0.40313783359814837 accuracy 0.7387480735778809\n",
      "Val loss 4.60586895942688 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 414/500\n",
      "Train loss 0.40542838264660663 accuracy 0.7381807565689087\n",
      "Val loss 4.307534861564636 accuracy 0.7117346525192261\n",
      "----------\n",
      "Epoch 415/500\n",
      "Train loss 0.4065661904323532 accuracy 0.7355332374572754\n",
      "Val loss 4.489814829826355 accuracy 0.7125850319862366\n",
      "----------\n",
      "Epoch 416/500\n",
      "Train loss 0.412231651774372 accuracy 0.7332639694213867\n",
      "Val loss 4.755581068992615 accuracy 0.7117346525192261\n",
      "----------\n",
      "Epoch 417/500\n",
      "Train loss 0.40615761675030354 accuracy 0.7354387044906616\n",
      "Val loss 4.871553063392639 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 418/500\n",
      "Train loss 0.4042073039405317 accuracy 0.7403554916381836\n",
      "Val loss 4.681118416786194 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 419/500\n",
      "Train loss 0.40390478523380785 accuracy 0.7395045161247253\n",
      "Val loss 4.573675966262817 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 420/500\n",
      "Train loss 0.4020145165633006 accuracy 0.7391263246536255\n",
      "Val loss 5.592280268669128 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 421/500\n",
      "Train loss 0.4044167140880263 accuracy 0.738369882106781\n",
      "Val loss 4.9481254577636715 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 422/500\n",
      "Train loss 0.40781249842011785 accuracy 0.7361005544662476\n",
      "Val loss 4.840548419952393 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 423/500\n",
      "Train loss 0.4077511813985296 accuracy 0.7308055758476257\n",
      "Val loss 4.718582034111023 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 424/500\n",
      "Train loss 0.4047468247183834 accuracy 0.7402609586715698\n",
      "Val loss 4.835215139389038 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 425/500\n",
      "Train loss 0.40083657223058033 accuracy 0.7407336831092834\n",
      "Val loss 5.2163913488388065 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 426/500\n",
      "Train loss 0.41218337452555276 accuracy 0.7338312864303589\n",
      "Val loss 5.28187370300293 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 427/500\n",
      "Train loss 0.4062467745269637 accuracy 0.7386535406112671\n",
      "Val loss 5.268008542060852 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 428/500\n",
      "Train loss 0.4102140647101115 accuracy 0.734114944934845\n",
      "Val loss 5.0321465015411375 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 429/500\n",
      "Train loss 0.4043141739914216 accuracy 0.73591148853302\n",
      "Val loss 5.3035375595092775 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 430/500\n",
      "Train loss 0.4071752457733614 accuracy 0.7378025650978088\n",
      "Val loss 5.493178868293763 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 431/500\n",
      "Train loss 0.40687586385083485 accuracy 0.7349659204483032\n",
      "Val loss 5.167158198356629 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 432/500\n",
      "Train loss 0.4091857858451016 accuracy 0.7346822619438171\n",
      "Val loss 5.107098150253296 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 433/500\n",
      "Train loss 0.40286511398223507 accuracy 0.7379916310310364\n",
      "Val loss 5.601652193069458 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 434/500\n",
      "Train loss 0.4045173640710762 accuracy 0.7373297810554504\n",
      "Val loss 5.0947486639022825 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 435/500\n",
      "Train loss 0.4039527167038745 accuracy 0.7379916310310364\n",
      "Val loss 5.844650363922119 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 436/500\n",
      "Train loss 0.4047126062663205 accuracy 0.7361005544662476\n",
      "Val loss 5.560367918014526 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 437/500\n",
      "Train loss 0.40731411849159793 accuracy 0.7353441715240479\n",
      "Val loss 5.195922255516052 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 438/500\n",
      "Train loss 0.40453348425497493 accuracy 0.7385589480400085\n",
      "Val loss 5.318271350860596 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 439/500\n",
      "Train loss 0.4058818759688412 accuracy 0.7353441715240479\n",
      "Val loss 5.02343533039093 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 440/500\n",
      "Train loss 0.40708829229136545 accuracy 0.7366678714752197\n",
      "Val loss 5.258318185806274 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 441/500\n",
      "Train loss 0.40372673538794 accuracy 0.7378025650978088\n",
      "Val loss 5.465633201599121 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 442/500\n",
      "Train loss 0.4053263958678188 accuracy 0.7384644150733948\n",
      "Val loss 5.201330184936523 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 443/500\n",
      "Train loss 0.40354320371007346 accuracy 0.7352495789527893\n",
      "Val loss 5.396926021575927 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 444/500\n",
      "Train loss 0.4073872756527131 accuracy 0.7342094779014587\n",
      "Val loss 5.56913914680481 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 445/500\n",
      "Train loss 0.40653063051671867 accuracy 0.7389371991157532\n",
      "Val loss 5.533263039588928 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 446/500\n",
      "Train loss 0.4022696014628353 accuracy 0.7415847182273865\n",
      "Val loss 5.92775936126709 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 447/500\n",
      "Train loss 0.4032481352248824 accuracy 0.7381807565689087\n",
      "Val loss 5.348915362358094 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 448/500\n",
      "Train loss 0.4048150964530118 accuracy 0.7376134395599365\n",
      "Val loss 5.485797333717346 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 449/500\n",
      "Train loss 0.4081100132091936 accuracy 0.7338312864303589\n",
      "Val loss 6.045610809326172 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 450/500\n",
      "Train loss 0.3993589034281581 accuracy 0.7398827075958252\n",
      "Val loss 5.805020761489868 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 451/500\n",
      "Train loss 0.40354509382362824 accuracy 0.7347768545150757\n",
      "Val loss 5.511165070533752 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 452/500\n",
      "Train loss 0.4078738628381706 accuracy 0.7361005544662476\n",
      "Val loss 5.42381911277771 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 453/500\n",
      "Train loss 0.40695534664464283 accuracy 0.7390317320823669\n",
      "Val loss 5.722333025932312 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 454/500\n",
      "Train loss 0.40648983436894703 accuracy 0.7351550459861755\n",
      "Val loss 5.648817753791809 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 455/500\n",
      "Train loss 0.4096556054540427 accuracy 0.7388426065444946\n",
      "Val loss 5.849688982963562 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 456/500\n",
      "Train loss 0.4037931593785803 accuracy 0.7377079725265503\n",
      "Val loss 6.050027680397034 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 457/500\n",
      "Train loss 0.40945714963487834 accuracy 0.7304273843765259\n",
      "Val loss 5.673115611076355 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 458/500\n",
      "Train loss 0.4051349062517465 accuracy 0.7379916310310364\n",
      "Val loss 6.081204986572265 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 459/500\n",
      "Train loss 0.39976040282881403 accuracy 0.7418683767318726\n",
      "Val loss 6.091202974319458 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 460/500\n",
      "Train loss 0.4051583312362073 accuracy 0.7348713874816895\n",
      "Val loss 5.802667045593262 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 461/500\n",
      "Train loss 0.4093104209526476 accuracy 0.7329803109169006\n",
      "Val loss 5.660267114639282 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 462/500\n",
      "Train loss 0.40750461026846646 accuracy 0.7339258193969727\n",
      "Val loss 5.797448992729187 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 463/500\n",
      "Train loss 0.4021503469312047 accuracy 0.7421520352363586\n",
      "Val loss 6.025665736198425 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 464/500\n",
      "Train loss 0.405523496938039 accuracy 0.7381807565689087\n",
      "Val loss 6.396124649047851 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 465/500\n",
      "Train loss 0.3976559710789876 accuracy 0.7447994947433472\n",
      "Val loss 6.421615123748779 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 466/500\n",
      "Train loss 0.4054904964314886 accuracy 0.7379916310310364\n",
      "Val loss 6.151040506362915 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 467/500\n",
      "Train loss 0.40256847243711175 accuracy 0.739315390586853\n",
      "Val loss 6.020989418029785 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 468/500\n",
      "Train loss 0.410887854285987 accuracy 0.7333585023880005\n",
      "Val loss 5.8631370782852175 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 469/500\n",
      "Train loss 0.4098988107169967 accuracy 0.7329803109169006\n",
      "Val loss 6.0537378787994385 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 470/500\n",
      "Train loss 0.4049218997179744 accuracy 0.7378025650978088\n",
      "Val loss 6.431778454780579 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 471/500\n",
      "Train loss 0.4069631149969905 accuracy 0.7355332374572754\n",
      "Val loss 5.870224118232727 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 472/500\n",
      "Train loss 0.40874059408544056 accuracy 0.7342094779014587\n",
      "Val loss 7.027562522888184 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 473/500\n",
      "Train loss 0.4057388427745865 accuracy 0.7375189065933228\n",
      "Val loss 6.101516151428223 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 474/500\n",
      "Train loss 0.40904424635760755 accuracy 0.7335476279258728\n",
      "Val loss 6.444204044342041 accuracy 0.7108843326568604\n",
      "----------\n",
      "Epoch 475/500\n",
      "Train loss 0.40793008222637406 accuracy 0.734398603439331\n",
      "Val loss 6.074028062820434 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 476/500\n",
      "Train loss 0.40318628427494 accuracy 0.7387480735778809\n",
      "Val loss 6.285554075241089 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 477/500\n",
      "Train loss 0.4069458616067128 accuracy 0.7357223629951477\n",
      "Val loss 6.383393669128418 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 478/500\n",
      "Train loss 0.403728494084025 accuracy 0.7354387044906616\n",
      "Val loss 6.387219429016113 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 479/500\n",
      "Train loss 0.4090568434043103 accuracy 0.7325075268745422\n",
      "Val loss 6.291801524162293 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 480/500\n",
      "Train loss 0.40083366596555137 accuracy 0.7411119341850281\n",
      "Val loss 6.072128796577454 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 481/500\n",
      "Train loss 0.4051567804382508 accuracy 0.7380862236022949\n",
      "Val loss 6.379633688926697 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 482/500\n",
      "Train loss 0.40399131753358497 accuracy 0.7360060214996338\n",
      "Val loss 6.829452419281006 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 483/500\n",
      "Train loss 0.40204851060028535 accuracy 0.7373297810554504\n",
      "Val loss 6.551159763336182 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 484/500\n",
      "Train loss 0.4087834430028157 accuracy 0.73591148853302\n",
      "Val loss 6.53221869468689 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 485/500\n",
      "Train loss 0.4057890683053488 accuracy 0.7395990490913391\n",
      "Val loss 6.377412176132202 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 486/500\n",
      "Train loss 0.4060785400580211 accuracy 0.7369515895843506\n",
      "Val loss 6.239564371109009 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 487/500\n",
      "Train loss 0.4052752726767437 accuracy 0.7370461225509644\n",
      "Val loss 6.282449460029602 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 488/500\n",
      "Train loss 0.4049997085548309 accuracy 0.7361005544662476\n",
      "Val loss 6.267263603210449 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 489/500\n",
      "Train loss 0.4089576724063919 accuracy 0.7344931364059448\n",
      "Val loss 6.682318711280823 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 490/500\n",
      "Train loss 0.4132421659417899 accuracy 0.731656551361084\n",
      "Val loss 6.3808036804199215 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 491/500\n",
      "Train loss 0.40524280250790606 accuracy 0.734114944934845\n",
      "Val loss 6.6248891115188595 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 492/500\n",
      "Train loss 0.4113622778151409 accuracy 0.7303327918052673\n",
      "Val loss 6.7642639636993405 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 493/500\n",
      "Train loss 0.41163425345018684 accuracy 0.7321293354034424\n",
      "Val loss 6.255395293235779 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 494/500\n",
      "Train loss 0.4060808244239853 accuracy 0.7367624640464783\n",
      "Val loss 6.542151689529419 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 495/500\n",
      "Train loss 0.40083891512399694 accuracy 0.7420574426651001\n",
      "Val loss 6.221905899047852 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 496/500\n",
      "Train loss 0.4032037778791175 accuracy 0.7397881746292114\n",
      "Val loss 6.219826209545135 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 497/500\n",
      "Train loss 0.4089144127196576 accuracy 0.7345877289772034\n",
      "Val loss 6.352725863456726 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 498/500\n",
      "Train loss 0.4092893639960921 accuracy 0.7347768545150757\n",
      "Val loss 6.68515911102295 accuracy 0.7091836333274841\n",
      "----------\n",
      "Epoch 499/500\n",
      "Train loss 0.41255871969533253 accuracy 0.7361951470375061\n",
      "Val loss 6.523680067062378 accuracy 0.7100340127944946\n",
      "----------\n",
      "Epoch 500/500\n",
      "Train loss 0.4013688618160156 accuracy 0.7402609586715698\n",
      "Val loss 6.598443126678466 accuracy 0.7091836333274841\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model.best_acc = 0\n",
    "model.vocab = vocab\n",
    "\n",
    "simple_weight_path = \"best_simple.pt\"\n",
    "\n",
    "train_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model = model.train()\n",
    "    correct_predictions, train_loss = simple_train_epoch(model, optimizer, train_loader, loss_function, device)\n",
    "    train_acc = correct_predictions / len(train_data)\n",
    "    train_acc = train_acc.item()\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    model = model.eval()\n",
    "    correct_predictions, val_loss = simple_eval_epoch(model, valid_loader, loss_function, device)\n",
    "    current_acc = correct_predictions / len(valid_data)\n",
    "    current_acc = current_acc.item()\n",
    "    print(f'Val loss {val_loss} accuracy {current_acc}')\n",
    "\n",
    "    train_history.append([train_loss, val_loss, train_acc, current_acc])\n",
    "\n",
    "    if current_acc > model.best_acc:\n",
    "        model.best_acc = current_acc\n",
    "        torch.save(model, simple_weight_path)\n",
    "\n",
    "    print('-' * 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_history = np.array(train_history)\n",
    "plt.plot(train_history[:, 2], label=\"train acc\")\n",
    "plt.plot(train_history[:, 3], label=\"val acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Выводы\n",
    "Модель слишком лёгкая для такой задачи, зато быстрая"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Попробовать стандартный пайплайн TF-IDF + Логистическая регрессия"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "out = tfidf.fit_transform(np.array(new_df[\"text\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 самых частых и самых редких слов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(array(['эт', 'прост', 'так', 'котор', 'дела', 'нужн', 'работа', 'дан',\n        'одн', 'сдела', 'вообщ', 'сам', 'дума', 'говор', 'теб', 'код',\n        'как', 'друг', 'питон', 'вопрос'], dtype=object),\n array(['00907', 'right', 'excelsheet', 'verbos', 'projectnumb', 'lost',\n        'prefix', 'volatil', 'reboot', 'persist', 'sinc', 'suffici',\n        'mkdir', 'nbi', 'tmpfile', 'python37', 'блджад', 'dynload',\n        'bearer', 'nпод'], dtype=object))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_idx = np.ravel(out.sum(axis=0).argsort(axis=1))[::-1]\n",
    "np.array(tfidf.get_feature_names_out())[sort_idx][:20], np.array(tfidf.get_feature_names_out())[sort_idx][-20:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "x_train_tf = tfidf.transform(x_train)\n",
    "x_test_tf = tfidf.transform(x_test)\n",
    "x_valid_tf = tfidf.transform(x_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "({'C': 2.0}, 0.7762858548921957)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "\n",
    "params = {\n",
    "    'C': np.arange(0.5, 2.6, 0.25)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, params, n_jobs=-1, cv=3\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train_tf, y_train)\n",
    "\n",
    "grid_search.best_params_, grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42, n_jobs=-1, C=2).fit(x_train_tf, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты на train:\n",
      "accuracy = 0.9215204236006052\n",
      "precision = 0.9536705842275592        \n",
      "recall = 0.9093714471333442\n",
      "f1 = 0.9309943465247755\n",
      "----------\n",
      "Результаты на val:\n",
      "accuracy = 0.8061389337641357\n",
      "precision = 0.877906976744186        \n",
      "recall = 0.7947368421052632\n",
      "f1 = 0.8342541436464088\n",
      "----------\n",
      "Результаты на test:\n",
      "accuracy = 0.7967687074829932\n",
      "precision = 0.886676875957121        \n",
      "recall = 0.7782258064516129\n",
      "f1 = 0.8289191123836793\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for name, data, target in zip([\"train\", \"val\", \"test\"], [x_train_tf, x_test_tf, x_valid_tf], [y_train, y_test, y_valid]):\n",
    "    predict = model.predict(data)\n",
    "\n",
    "    print(f\"Результаты на {name}:\")\n",
    "    print_score(predict, target)\n",
    "\n",
    "    print(\"-\" * 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "pickle.dump(tfidf, open('tfidf.pickle', 'wb'))\n",
    "pickle.dump(model, open('logreg.pickle', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Итог\n",
    "Лучше всего себя показал модель на TF-IDF, и по скорости и по качеству. Есть ощущение, что из трансформеров можно выжать сильно больше.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
